{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85cd6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')  # Non-interactive backend for HPC\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "037d4ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(log_dir='logs', experiment_name='smr_seld'):\n",
    "    \"\"\"Setup comprehensive logging for HPC environment\"\"\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    log_file = os.path.join(log_dir, f'{experiment_name}_{timestamp}.log')\n",
    "    \n",
    "    # Create logger\n",
    "    logger = logging.getLogger('SMR_SELD')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Clear existing handlers to prevent duplicates when re-running cells\n",
    "    if logger.handlers:\n",
    "        logger.handlers.clear()\n",
    "    \n",
    "    # File handler\n",
    "    fh = logging.FileHandler(log_file)\n",
    "    fh.setLevel(logging.INFO)\n",
    "    \n",
    "    # Console handler\n",
    "    ch = logging.StreamHandler(sys.stdout)\n",
    "    ch.setLevel(logging.INFO)\n",
    "    \n",
    "    # Formatter\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    fh.setFormatter(formatter)\n",
    "    ch.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "    \n",
    "    return logger, log_file\n",
    "\n",
    "logger, log_file = setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa46e705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-17 23:10:38 - SMR_SELD - INFO - CUDA is available! Using GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "2025-11-17 23:10:38 - SMR_SELD - INFO - CUDA Version: 11.8\n",
      "2025-11-17 23:10:38 - SMR_SELD - INFO - GPU Memory: 4.00 GB\n",
      "2025-11-17 23:10:38 - SMR_SELD - INFO - CUDA Version: 11.8\n",
      "2025-11-17 23:10:38 - SMR_SELD - INFO - GPU Memory: 4.00 GB\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    \"\"\"Get available device with CUDA support\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        logger.info(f\"CUDA is available! Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        logger.info(f\"CUDA Version: {torch.version.cuda}\")\n",
    "        logger.info(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        logger.warning(\"CUDA not available. Using CPU. Training will be slower.\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df2dfb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration class for SMR-SELD training\"\"\"\n",
    "    \n",
    "    # Paths (relative to script location)\n",
    "    BASE_PATH = Path.cwd()\n",
    "    AUDIO_PATH = BASE_PATH / \"foa_dev\"\n",
    "    METADATA_PATH = BASE_PATH / \"metadata_dev\"\n",
    "    OUTPUT_PATH = BASE_PATH / \"outputs\"\n",
    "    CHECKPOINT_PATH = BASE_PATH / \"checkpoints\"\n",
    "    \n",
    "    # Dataset - Use full dataset or single file for testing\n",
    "    USE_FULL_DATASET = True  # Set to False for quick testing with single file\n",
    "    TRAIN_AUDIO_FILE = \"fold3_room21_mix001.wav\"  # Used only if USE_FULL_DATASET=False\n",
    "    TRAIN_META_FILE = \"fold3_room21_mix001.csv\"\n",
    "    TEST_AUDIO_FILE = \"fold4_room23_mix001.wav\"\n",
    "    TEST_META_FILE = \"fold4_room23_mix001.csv\"\n",
    "    \n",
    "    # STARSS22 Classes\n",
    "    STARSS22_CLASSES = {\n",
    "        0: 'Female speech, woman speaking',\n",
    "        1: 'Male speech, man speaking',\n",
    "        2: 'Clapping',\n",
    "        3: 'Telephone',\n",
    "        4: 'Laughter',\n",
    "        5: 'Domestic sounds',\n",
    "        6: 'Walk, footsteps',\n",
    "        7: 'Door, open or close',\n",
    "        8: 'Music',\n",
    "        9: 'Musical instrument',\n",
    "        10: 'Water tap, faucet',\n",
    "        11: 'Bell',\n",
    "        12: 'Knock',\n",
    "        13: 'Background'\n",
    "    }\n",
    "    \n",
    "    # Model\n",
    "    NUM_CLASSES = 14  # 13 classes + 1 background\n",
    "    N_CHANNELS = 4  # FOA channels    \n",
    "    \n",
    "    # Training hyperparameters\n",
    "    NUM_EPOCHS = 25\n",
    "    BATCH_SIZE = 8\n",
    "    LEARNING_RATE = 1e-3\n",
    "    LR_DECAY_FACTOR = 0.5  # Multiply LR by this factor\n",
    "    LR_DECAY_PATIENCE = 5   # Decay LR after this many epochs without improvement\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    \n",
    "    # Loss weights\n",
    "    W_CLASS = 1.0\n",
    "    W_AIUR = 0.5\n",
    "    W_CL = 0.5\n",
    "    \n",
    "    # Early stopping\n",
    "    PATIENCE = 10\n",
    "    MIN_DELTA = 1e-4\n",
    "    \n",
    "    # Checkpointing\n",
    "    SAVE_EVERY_N_EPOCHS = 5\n",
    "    KEEP_LAST_N_CHECKPOINTS = 3\n",
    "    \n",
    "    # Signal Processing (to convert into spectro)\n",
    "    SPECTROGRAM_N_FFT = int(0.04*24000)\n",
    "    SPECTROGRAM_HOP_LENGTH = int(0.02*24000) \n",
    "    N_MELS = 64\n",
    "    SR = 24000\n",
    "    \n",
    "    # Dataset\n",
    "    # 5s window (saved as no of frames) to feed into the model\n",
    "    #1s hop length (converted to no of frames) to feed into the model\n",
    "    WINDOW_LENGTH = int(5*24000)\n",
    "    HOP_LENGTH = int(1*24000)\n",
    "    \n",
    "    # 3D to 2D Mapping\n",
    "    I = None\n",
    "    J = None\n",
    "    GRID_CELL_DEGREES = 10\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        # Create directories\n",
    "        self.OUTPUT_PATH.mkdir(exist_ok=True, parents=True)\n",
    "        self.CHECKPOINT_PATH.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        # Build full paths (used only for single file mode)\n",
    "        self.TRAIN_AUDIO_PATH = self.AUDIO_PATH / \"dev-train-sony\" / self.TRAIN_AUDIO_FILE\n",
    "        self.TRAIN_META_PATH = self.METADATA_PATH / \"dev-train-sony\" / self.TRAIN_META_FILE\n",
    "        self.TEST_AUDIO_PATH = self.AUDIO_PATH / \"dev-test-sony\" / self.TEST_AUDIO_FILE\n",
    "        self.TEST_META_PATH = self.METADATA_PATH / \"dev-test-sony\" / self.TEST_META_FILE\n",
    "        \n",
    "        # Dataset directories\n",
    "        self.SONY_TRAIN_DIR = self.AUDIO_PATH / \"dev-train-sony\"\n",
    "        self.SONY_TEST_DIR = self.AUDIO_PATH / \"dev-test-sony\"\n",
    "        self.SONY_TRAIN_META_DIR = self.METADATA_PATH / \"dev-train-sony\"\n",
    "        self.SONY_TEST_META_DIR = self.METADATA_PATH / \"dev-test-sony\"\n",
    "        self.TAU_TRAIN_DIR = self.AUDIO_PATH / \"dev-train-tau\"\n",
    "        self.TAU_TEST_DIR = self.AUDIO_PATH / \"dev-test-tau\"\n",
    "        self.TAU_TRAIN_META_DIR = self.METADATA_PATH / \"dev-train-tau\"\n",
    "        self.TAU_TEST_META_DIR = self.METADATA_PATH / \"dev-test-tau\"\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ad24c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files():\n",
    "    \"\"\"Load audio and metadata files based on configuration\"\"\"\n",
    "    if config.USE_FULL_DATASET:\n",
    "        # Load all audio files\n",
    "        sony_train_audio = sorted(glob(str(config.SONY_TRAIN_DIR / \"*.wav\")))\n",
    "        tau_train_audio = sorted(glob(str(config.TAU_TRAIN_DIR / \"*.wav\")))\n",
    "        sony_test_audio = sorted(glob(str(config.SONY_TEST_DIR / \"*.wav\")))\n",
    "        tau_test_audio = sorted(glob(str(config.TAU_TEST_DIR / \"*.wav\")))\n",
    "        \n",
    "        # Match metadata files to audio files by basename\n",
    "        def get_matching_metadata(audio_files, meta_dir):\n",
    "            \"\"\"Get metadata files matching audio files by basename\"\"\"\n",
    "            meta_files = []\n",
    "            for audio_file in audio_files:\n",
    "                # Get basename without extension (e.g., fold3_room21_mix001)\n",
    "                basename = Path(audio_file).stem\n",
    "                # Build corresponding metadata path\n",
    "                meta_file = meta_dir / f\"{basename}.csv\"\n",
    "                if meta_file.exists():\n",
    "                    meta_files.append(str(meta_file))\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"Metadata file not found: {meta_file}\")\n",
    "            return meta_files\n",
    "        \n",
    "        # Get matching metadata files\n",
    "        sony_train_meta = get_matching_metadata(sony_train_audio, config.SONY_TRAIN_META_DIR)\n",
    "        tau_train_meta = get_matching_metadata(tau_train_audio, config.TAU_TRAIN_META_DIR)\n",
    "        sony_test_meta = get_matching_metadata(sony_test_audio, config.SONY_TEST_META_DIR)\n",
    "        tau_test_meta = get_matching_metadata(tau_test_audio, config.TAU_TEST_META_DIR)\n",
    "        \n",
    "        # Combine training and testing files\n",
    "        train_audio_files = sony_train_audio + tau_train_audio\n",
    "        train_meta_files = sony_train_meta + tau_train_meta\n",
    "        test_audio_files = sony_test_audio + tau_test_audio\n",
    "        test_meta_files = sony_test_meta + tau_test_meta\n",
    "    else:\n",
    "        # Load single training file\n",
    "        train_audio_files = [str(config.TRAIN_AUDIO_PATH)]\n",
    "        train_meta_files = [str(config.TRAIN_META_PATH)]\n",
    "        \n",
    "        # Load single testing file\n",
    "        test_audio_files = [str(config.TEST_AUDIO_PATH)]\n",
    "        test_meta_files = [str(config.TEST_META_PATH)]\n",
    "    \n",
    "    return train_audio_files, train_meta_files, test_audio_files, test_meta_files\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ed7c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polar_to_grid(phi, theta, I=None, J=None, cell_size_deg=None):\n",
    "    \"\"\"\n",
    "    Convert polar coordinates (azimuth phi, elevation theta) to grid indices (i, j).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    phi : float\n",
    "        Azimuth in degrees (range [-180, 180]).\n",
    "    theta : float\n",
    "        Elevation in degrees (range [-90, 90]).\n",
    "    I : int, optional\n",
    "        Number of elevation bins. If None, computed from cell_size_deg.\n",
    "    J : int, optional\n",
    "        Number of azimuth bins. If None, computed from cell_size_deg.\n",
    "    cell_size_deg : float, optional\n",
    "        Size of each grid cell in degrees. Required if I or J is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    i, j : tuple of int\n",
    "        Grid row (elevation index) and column (azimuth index).\n",
    "    \"\"\"\n",
    "    # If grid dimensions not provided, compute from cell size\n",
    "    if (I is None or J is None) and cell_size_deg is not None:\n",
    "        I = int(180 // cell_size_deg)\n",
    "        J = int(360 // cell_size_deg)\n",
    "    elif I is None or J is None:\n",
    "        raise ValueError(\"Either provide (I, J) or cell_size_deg for polar_to_grid\")\n",
    "\n",
    "    # Normalize azimuth and elevation to [0,1]\n",
    "    phi_norm = (phi + 180.0) / 360.0\n",
    "    theta_norm = (theta + 90.0) / 180.0\n",
    "    j = int(np.clip(phi_norm * J, 0, J - 1))\n",
    "    i = int(np.clip(theta_norm * I, 0, I - 1))\n",
    "    return i, j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "903f1a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(audio_path):\n",
    "    \"\"\"\n",
    "    Load multi-channel audio file using torchaudio.\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to the audio file\n",
    "        \n",
    "    Returns:\n",
    "        waveform: Tensor of shape (channels, samples) - preserves all 4 FOA channels\n",
    "        sample_rate: Sample rate of the audio\n",
    "    \"\"\"\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Verify we have 4 channels (FOA format)\n",
    "    if waveform.shape[0] != 4:\n",
    "        logger.warning(f\"Expected 4 channels but got {waveform.shape[0]} channels in {audio_path}\")\n",
    "    \n",
    "    return waveform, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7327a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_mel_spectrogram(waveform, sample_rate, n_fft=None, hop_length=None, n_mels=None):\n",
    "    \"\"\"\n",
    "    Convert multi-channel audio waveform to mel spectrogram.\n",
    "    Processes each channel separately and stacks them.\n",
    "    \n",
    "    Args:\n",
    "        waveform: Tensor of shape (channels, samples) - typically (4, num_samples) for FOA\n",
    "        sample_rate: Sample rate of the audio\n",
    "        n_fft: FFT window size (default: from config)\n",
    "        hop_length: Hop length for STFT (default: from config)\n",
    "        n_mels: Number of mel filterbanks (default: from config)\n",
    "        \n",
    "    Returns:\n",
    "        mel_spec: Tensor of shape (channels, n_mels, time_frames)\n",
    "    \"\"\"\n",
    "    # Use config defaults if not provided\n",
    "    if n_fft is None:\n",
    "        n_fft = config.SPECTROGRAM_N_FFT\n",
    "    if hop_length is None:\n",
    "        hop_length = config.SPECTROGRAM_HOP_LENGTH\n",
    "    if n_mels is None:\n",
    "        n_mels = config.N_MELS\n",
    "    \n",
    "    # Create mel spectrogram transform\n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels\n",
    "    )\n",
    "    \n",
    "    # Process each channel separately\n",
    "    mel_specs = []\n",
    "    for channel_idx in range(waveform.shape[0]):\n",
    "        channel_waveform = waveform[channel_idx:channel_idx+1, :]  # Keep dimension (1, samples)\n",
    "        mel_spec = mel_transform(channel_waveform)  # Shape: (1, n_mels, time_frames)\n",
    "        mel_specs.append(mel_spec)\n",
    "    \n",
    "    # Stack all channels: (channels, n_mels, time_frames)\n",
    "    mel_spec_multichannel = torch.cat(mel_specs, dim=0)\n",
    "    \n",
    "    # Convert to log scale (dB)\n",
    "    mel_spec_db = torchaudio.transforms.AmplitudeToDB()(mel_spec_multichannel)\n",
    "    \n",
    "    return mel_spec_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28813587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mel_spectrogram(mel_spec, title=\"Multi-Channel Mel Spectrogram\", figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Visualize multi-channel mel spectrogram.\n",
    "    \n",
    "    Args:\n",
    "        mel_spec: Tensor of shape (channels, n_mels, time_frames)\n",
    "        title: Title for the figure\n",
    "        figsize: Figure size (width, height)\n",
    "    \"\"\"\n",
    "    # Convert to numpy if it's a torch tensor\n",
    "    if isinstance(mel_spec, torch.Tensor):\n",
    "        mel_spec_np = mel_spec.cpu().numpy()\n",
    "    else:\n",
    "        mel_spec_np = mel_spec\n",
    "    \n",
    "    n_channels = mel_spec_np.shape[0]\n",
    "    \n",
    "    # Create subplots for each channel\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    # Flatten axes for easier iteration\n",
    "    axes_flat = axes.flatten()\n",
    "    \n",
    "    # Plot each channel\n",
    "    for i in range(n_channels):\n",
    "        ax = axes_flat[i]\n",
    "        im = ax.imshow(\n",
    "            mel_spec_np[i], \n",
    "            aspect='auto', \n",
    "            origin='lower',\n",
    "            cmap='viridis',\n",
    "            interpolation='nearest'\n",
    "        )\n",
    "        ax.set_title(f'Channel {i+1}')\n",
    "        ax.set_xlabel('Time Frames')\n",
    "        ax.set_ylabel('Mel Frequency Bins')\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(im, ax=ax, format='%+2.0f dB')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "456b50fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_to_labels(metadata_path, audio_duration, sample_rate=24000, I=None, J=None, \n",
    "                        cell_size_deg=None, num_classes=14):\n",
    "    \"\"\"\n",
    "    Convert metadata file to target labels for 2D grid representation.\n",
    "    \n",
    "    Args:\n",
    "        metadata_path: Path to the CSV metadata file\n",
    "        audio_duration: Duration of audio in seconds\n",
    "        sample_rate: Sample rate of audio (default: 24000 Hz)\n",
    "        I: Number of elevation bins (height of grid)\n",
    "        J: Number of azimuth bins (width of grid)\n",
    "        cell_size_deg: Cell size in degrees (alternative to I, J)\n",
    "        num_classes: Total number of classes including background (default: 14)\n",
    "        \n",
    "    Returns:\n",
    "        labels: Tensor of shape [T', I*J, M] where:\n",
    "                T' = number of 20ms frames\n",
    "                I*J = total grid cells\n",
    "                M = number of classes (14)\n",
    "    \"\"\"\n",
    "    # Use config default if not provided\n",
    "    if cell_size_deg is None:\n",
    "        cell_size_deg = config.GRID_CELL_DEGREES\n",
    "    \n",
    "    # Step 1: Calculate total number of frames (20ms per frame)\n",
    "    frame_duration_ms = 20  # 20ms per frame in final representation\n",
    "    metadata_frame_duration_ms = 100  # 100ms per frame in metadata\n",
    "    frames_per_metadata_frame = metadata_frame_duration_ms // frame_duration_ms  # = 5\n",
    "    \n",
    "    # Total number of frames for the audio\n",
    "    total_frames = int((audio_duration * 1000) / frame_duration_ms)\n",
    "    \n",
    "    # Calculate grid dimensions if not provided\n",
    "    if (I is None or J is None) and cell_size_deg is not None:\n",
    "        I = int(180 // cell_size_deg)\n",
    "        J = int(360 // cell_size_deg)\n",
    "    elif I is None or J is None:\n",
    "        raise ValueError(\"Either provide (I, J) or cell_size_deg for grid dimensions\")\n",
    "    \n",
    "    total_cells = I * J\n",
    "    \n",
    "    # Initialize labels tensor: [T', I*J, M] with all zeros\n",
    "    labels = torch.zeros((total_frames, total_cells, num_classes), dtype=torch.float32)\n",
    "    \n",
    "    # Read metadata CSV\n",
    "    df = pd.read_csv(metadata_path, header=None)\n",
    "    \n",
    "    # Track which cells have active events for each frame\n",
    "    # This will help us set background (class 13) for empty cells\n",
    "    active_cells_per_frame = [set() for _ in range(total_frames)]\n",
    "    \n",
    "    # Process each row in metadata\n",
    "    for _, row in df.iterrows():\n",
    "        # Parse metadata row: [frame, class, source, azimuth, elevation]\n",
    "        metadata_frame = int(row.iloc[0])  # Frame number from metadata\n",
    "        active_class = int(row.iloc[1])     # Active class index\n",
    "        source_num = int(row.iloc[2])       # Source number (not used in labeling)\n",
    "        azimuth = int(row.iloc[3])          # Azimuth in degrees\n",
    "        elevation = int(row.iloc[4])        # Elevation in degrees\n",
    "        \n",
    "        # Step 2: Map metadata frame to final representation frames\n",
    "        # Metadata frame t corresponds to frames t*5 to t*5+4 in final representation\n",
    "        start_frame = metadata_frame * frames_per_metadata_frame\n",
    "        end_frame = start_frame + frames_per_metadata_frame\n",
    "        \n",
    "        # Ensure we don't exceed total frames\n",
    "        end_frame = min(end_frame, total_frames)\n",
    "        \n",
    "        # Step 3: Convert polar coordinates to grid cell\n",
    "        i, j = polar_to_grid(azimuth, elevation, I=I, J=J)\n",
    "        cell_idx = i * J + j  # Flatten 2D grid to 1D index\n",
    "        \n",
    "        # Step 4: Set active class for this cell across the time frames\n",
    "        for t in range(start_frame, end_frame):\n",
    "            # Set the active class to 1 (one-hot encoding)\n",
    "            labels[t, cell_idx, active_class] = 1.0\n",
    "            # Track that this cell has an active event\n",
    "            active_cells_per_frame[t].add(cell_idx)\n",
    "    \n",
    "    # Step 5: Set background class (index 13) for cells with no active events\n",
    "    for t in range(total_frames):\n",
    "        for cell_idx in range(total_cells):\n",
    "            # If this cell has no active events in this frame\n",
    "            if cell_idx not in active_cells_per_frame[t]:\n",
    "                # Set background class (index 13) to 1\n",
    "                labels[t, cell_idx, num_classes - 1] = 1.0\n",
    "    \n",
    "    # Reshape to [T' * (I*J), M] if needed for certain loss functions\n",
    "    # For now, keep as [T', I*J, M] for clarity\n",
    "    \n",
    "    return labels, I, J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f598500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 67 54 54\n"
     ]
    }
   ],
   "source": [
    "train_audio_files, train_meta_files, test_audio_files, test_meta_files = load_files()\n",
    "print(len(train_audio_files), len(train_meta_files), len(test_audio_files), len(test_meta_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "329839e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SELDDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for Sound Event Localization and Detection (SELD).\n",
    "    \n",
    "    Loads all audio files, concatenates spectrograms and labels, then segments\n",
    "    into fixed-length windows with overlap for training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, audio_files, metadata_files, num_classes=14):\n",
    "        \"\"\"\n",
    "        Initialize SELD Dataset with windowing.\n",
    "        \n",
    "        This dataset:\n",
    "        1. Loads all audio files and computes spectrograms + labels\n",
    "        2. Concatenates all spectrograms and labels into single tensors\n",
    "        3. Segments concatenated data into windows (5s window, 1s hop)\n",
    "        4. Pads final window if needed\n",
    "        \n",
    "        Args:\n",
    "            audio_files: List of audio file paths\n",
    "            metadata_files: List of corresponding metadata CSV file paths\n",
    "            num_classes: Total number of classes including background (default: 14)\n",
    "        \"\"\"\n",
    "        assert len(audio_files) == len(metadata_files), \\\n",
    "            \"Number of audio files must match number of metadata files\"\n",
    "        \n",
    "        self.audio_files = audio_files\n",
    "        self.metadata_files = metadata_files\n",
    "        self.sample_rate = config.SR\n",
    "        self.n_fft = config.SPECTROGRAM_N_FFT\n",
    "        self.spectrogram_hop_length = config.SPECTROGRAM_HOP_LENGTH\n",
    "        self.n_mels = config.N_MELS\n",
    "        self.cell_size_deg = config.GRID_CELL_DEGREES\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        self.I = int(180 // self.cell_size_deg)\n",
    "        self.J = int(360 // self.cell_size_deg)\n",
    "        self.total_cells = self.I * self.J\n",
    "        \n",
    "        # Window parameters (in samples)\n",
    "        self.window_length_samples = config.WINDOW_LENGTH  # 5s in samples\n",
    "        self.hop_length_samples = config.HOP_LENGTH  # 1s in samples\n",
    "        \n",
    "        # Convert to spectrogram frames\n",
    "        # Each spectrogram frame represents spectrogram_hop_length samples\n",
    "        self.window_length_frames = int(self.window_length_samples / self.spectrogram_hop_length)\n",
    "        self.hop_length_frames = int(self.hop_length_samples / self.spectrogram_hop_length)\n",
    "        \n",
    "        logger.info(f\"SELDDataset initialization started...\")\n",
    "        logger.info(f\"  Files: {len(audio_files)} audio files\")\n",
    "        logger.info(f\"  Grid: {self.I}x{self.J} = {self.total_cells} cells\")\n",
    "        logger.info(f\"  Window: {self.window_length_frames} frames ({self.window_length_samples / self.sample_rate:.1f}s)\")\n",
    "        logger.info(f\"  Hop: {self.hop_length_frames} frames ({self.hop_length_samples / self.sample_rate:.1f}s)\")\n",
    "        \n",
    "        # Step 1 & 2: Load all files and concatenate\n",
    "        self._load_and_concatenate_all()\n",
    "        \n",
    "        # Step 3 & 4: Segment into windows\n",
    "        self._create_windows()\n",
    "        \n",
    "        logger.info(f\"SELDDataset initialized with {len(self.windows)} windows\")\n",
    "    \n",
    "    def _load_and_concatenate_all(self):\n",
    "        \"\"\"Load all files, compute spectrograms and labels, then concatenate.\"\"\"\n",
    "        all_spectrograms = []\n",
    "        all_labels = []\n",
    "        \n",
    "        logger.info(\"Loading and processing all audio files...\")\n",
    "        for idx, (audio_path, metadata_path) in enumerate(tqdm(\n",
    "            zip(self.audio_files, self.metadata_files),\n",
    "            total=len(self.audio_files),\n",
    "            desc=\"Processing files\"\n",
    "        )):\n",
    "            try:\n",
    "                # Load audio\n",
    "                waveform, sr = load_audio(audio_path)\n",
    "                \n",
    "                # Compute mel spectrogram\n",
    "                mel_spec = audio_to_mel_spectrogram(\n",
    "                    waveform, \n",
    "                    sr,\n",
    "                    n_fft=self.n_fft,\n",
    "                    hop_length=self.spectrogram_hop_length,\n",
    "                    n_mels=self.n_mels\n",
    "                )  # Shape: (4, n_mels, time_frames)\n",
    "                \n",
    "                # Calculate audio duration\n",
    "                audio_duration = waveform.shape[1] / sr\n",
    "                \n",
    "                # Generate labels from metadata\n",
    "                labels, _, _ = metadata_to_labels(\n",
    "                    metadata_path,\n",
    "                    audio_duration,\n",
    "                    sample_rate=sr,\n",
    "                    I=self.I,\n",
    "                    J=self.J,\n",
    "                    cell_size_deg=self.cell_size_deg,\n",
    "                    num_classes=self.num_classes\n",
    "                )  # Shape: (time_frames, I*J, num_classes)\n",
    "                \n",
    "                # Ensure matching time dimensions\n",
    "                mel_time_frames = mel_spec.shape[2]\n",
    "                label_time_frames = labels.shape[0]\n",
    "                \n",
    "                if mel_time_frames != label_time_frames:\n",
    "                    min_frames = min(mel_time_frames, label_time_frames)\n",
    "                    mel_spec = mel_spec[:, :, :min_frames]\n",
    "                    labels = labels[:min_frames, :, :]\n",
    "                \n",
    "                # Append to lists\n",
    "                all_spectrograms.append(mel_spec)\n",
    "                all_labels.append(labels)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing file {idx} ({audio_path}): {str(e)}\")\n",
    "                raise\n",
    "        \n",
    "        # Concatenate along time dimension\n",
    "        self.concatenated_spectrograms = torch.cat(all_spectrograms, dim=2)  # (4, n_mels, T)\n",
    "        self.concatenated_labels = torch.cat(all_labels, dim=0)  # (T, I*J, num_classes)\n",
    "        \n",
    "        self.total_frames = self.concatenated_spectrograms.shape[2]\n",
    "        logger.info(f\"Concatenated data: {self.total_frames} total frames\")\n",
    "        logger.info(f\"  Spectrograms shape: {self.concatenated_spectrograms.shape}\")\n",
    "        logger.info(f\"  Labels shape: {self.concatenated_labels.shape}\")\n",
    "    \n",
    "    def _create_windows(self):\n",
    "        \"\"\"Segment concatenated data into windows with overlap.\"\"\"\n",
    "        self.windows = []\n",
    "        \n",
    "        start_frame = 0\n",
    "        window_idx = 0\n",
    "        \n",
    "        while start_frame < self.total_frames:\n",
    "            end_frame = start_frame + self.window_length_frames\n",
    "            \n",
    "            # Extract window\n",
    "            if end_frame <= self.total_frames:\n",
    "                # Normal window - no padding needed\n",
    "                window_spec = self.concatenated_spectrograms[:, :, start_frame:end_frame]\n",
    "                window_labels = self.concatenated_labels[start_frame:end_frame, :, :]\n",
    "            else:\n",
    "                # Last window - needs padding\n",
    "                actual_frames = self.total_frames - start_frame\n",
    "                \n",
    "                # Extract what we have\n",
    "                window_spec = self.concatenated_spectrograms[:, :, start_frame:]\n",
    "                window_labels = self.concatenated_labels[start_frame:, :, :]\n",
    "                \n",
    "                # Pad to window_length_frames\n",
    "                pad_frames = self.window_length_frames - actual_frames\n",
    "                \n",
    "                # Pad spectrograms: (4, n_mels, time) -> pad time dimension\n",
    "                spec_pad = torch.zeros((4, self.n_mels, pad_frames), dtype=window_spec.dtype)\n",
    "                window_spec = torch.cat([window_spec, spec_pad], dim=2)\n",
    "                \n",
    "                # Pad labels: (time, I*J, num_classes) -> pad time dimension\n",
    "                # Set background class (index 13) for padded frames\n",
    "                label_pad = torch.zeros((pad_frames, self.total_cells, self.num_classes), dtype=window_labels.dtype)\n",
    "                label_pad[:, :, self.num_classes - 1] = 1.0  # Set background class\n",
    "                window_labels = torch.cat([window_labels, label_pad], dim=0)\n",
    "            \n",
    "            # Transpose spectrogram from [C, F, T] to [T, C, F]\n",
    "            window_spec = window_spec.permute(2, 0, 1)  # (T, C, F)\n",
    "            \n",
    "            # Store window\n",
    "            self.windows.append({\n",
    "                'spectrogram': window_spec,\n",
    "                'labels': window_labels,\n",
    "                'window_idx': window_idx,\n",
    "                'start_frame': start_frame,\n",
    "                'end_frame': min(end_frame, self.total_frames)\n",
    "            })\n",
    "            \n",
    "            # Move to next window\n",
    "            start_frame += self.hop_length_frames\n",
    "            window_idx += 1\n",
    "        \n",
    "        logger.info(f\"Created {len(self.windows)} windows\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of windows in the dataset.\"\"\"\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single window from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            idx: Window index\n",
    "            \n",
    "        Returns:\n",
    "            spectrogram: Mel spectrogram tensor of shape (window_length_frames, 4, n_mels) - [T, C, F]\n",
    "            labels: Target labels tensor of shape (window_length_frames, I*J, num_classes) - [T, I*J, M]\n",
    "        \"\"\"\n",
    "        window = self.windows[idx]\n",
    "        return window['spectrogram'], window['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf0e51",
   "metadata": {},
   "source": [
    "## SELDDataset with Windowing\n",
    "\n",
    "The `SELDDataset` class implements a windowing approach for SELD training:\n",
    "\n",
    "### Workflow:\n",
    "1. **Load all files**: Each audio file is loaded and processed to extract:\n",
    "   - Mel spectrogram: shape `(4, 64, T_i)` for file i\n",
    "   - Labels: shape `(T_i, 648, 14)` for file i\n",
    "\n",
    "2. **Concatenate**: All spectrograms and labels are concatenated along the time dimension:\n",
    "   - Concatenated spectrogram: `(4, 64, T)` where T = sum of all T_i\n",
    "   - Concatenated labels: `(T, 648, 14)`\n",
    "\n",
    "3. **Segment into windows**: The concatenated data is segmented into fixed-length windows:\n",
    "   - Window length: 5 seconds = 250 frames (at 20ms per frame)\n",
    "   - Hop length: 1 second = 50 frames\n",
    "   - Overlap: 4 seconds (200 frames) between consecutive windows\n",
    "\n",
    "4. **Padding**: The final window is padded if it has fewer than 250 frames:\n",
    "   - Spectrograms: padded with zeros\n",
    "   - Labels: padded with background class (index 13 = 1.0)\n",
    "\n",
    "5. **Transpose**: Spectrograms are transposed from `[C, F, T]` to `[T, C, F]` format\n",
    "\n",
    "### Output:\n",
    "Each window provides:\n",
    "- Spectrogram: `(250, 4, 64)` - **[T, C, F]** format: 250 time frames, 4 channels, 64 mel bins\n",
    "- Labels: `(250, 648, 14)` - **[T, I×J, M]** format: 250 time frames, 648 spatial cells, 14 classes\n",
    "\n",
    "This ensures all training samples have consistent shapes and can be batched efficiently.\n",
    "\n",
    "This ensures all training samples have consistent shapes and can be batched efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5af2f",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e06a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    \"\"\"Standard convolution with BN and SiLU activation\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"Standard bottleneck block with residual connection\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, shortcut=True):\n",
    "        super().__init__()\n",
    "        self.cv1 = Conv(in_channels, out_channels, 1, 1, 0)\n",
    "        self.cv2 = Conv(out_channels, out_channels, 3, 1, 1)\n",
    "        self.add = shortcut and in_channels == out_channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
    "\n",
    "class C3(nn.Module):\n",
    "    \"\"\"CSP Bottleneck with 3 convolutions\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, n_blocks=1, shortcut=True):\n",
    "        super().__init__()\n",
    "        hidden_channels = out_channels // 2\n",
    "        self.cv1 = Conv(in_channels, hidden_channels, 1, 1, 0)\n",
    "        self.cv2 = Conv(in_channels, hidden_channels, 1, 1, 0)\n",
    "        self.cv3 = Conv(2 * hidden_channels, out_channels, 1, 1, 0)\n",
    "        self.m = nn.Sequential(\n",
    "            *[Bottleneck(hidden_channels, hidden_channels, shortcut) for _ in range(n_blocks)]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))\n",
    "\n",
    "class SPPF(nn.Module):\n",
    "    \"\"\"Spatial Pyramid Pooling - Fast\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5):\n",
    "        super().__init__()\n",
    "        hidden_channels = in_channels // 2\n",
    "        self.cv1 = Conv(in_channels, hidden_channels, 1, 1, 0)\n",
    "        self.cv2 = Conv(hidden_channels * 4, out_channels, 1, 1, 0)\n",
    "        self.m = nn.MaxPool2d(kernel_size=kernel_size, stride=1, padding=kernel_size // 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cv1(x)\n",
    "        y1 = self.m(x)\n",
    "        y2 = self.m(y1)\n",
    "        y3 = self.m(y2)\n",
    "        return self.cv2(torch.cat([x, y1, y2, y3], dim=1))\n",
    "\n",
    "class CSPDarkNet53(nn.Module):\n",
    "    \"\"\"CSPDarkNet53 backbone for audio SELD\"\"\"\n",
    "    def __init__(self, in_channels=4, base_channels=64, depth_multiple=1.0, width_multiple=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        def get_channels(c):\n",
    "            return max(round(c * width_multiple), 1)\n",
    "        \n",
    "        def get_depth(n):\n",
    "            return max(round(n * depth_multiple), 1)\n",
    "        \n",
    "        # 3x3 stem for audio\n",
    "        self.stem = Conv(in_channels, get_channels(base_channels), 3, 1, 1)\n",
    "        \n",
    "        self.stage1 = nn.Sequential(\n",
    "            Conv(get_channels(64), get_channels(128), 3, 2, 1),\n",
    "            C3(get_channels(128), get_channels(128), n_blocks=get_depth(3))\n",
    "        )\n",
    "        \n",
    "        self.stage2 = nn.Sequential(\n",
    "            Conv(get_channels(128), get_channels(256), 3, 2, 1),\n",
    "            C3(get_channels(256), get_channels(256), n_blocks=get_depth(6))\n",
    "        )\n",
    "        \n",
    "        self.stage3 = nn.Sequential(\n",
    "            Conv(get_channels(256), get_channels(512), 3, 2, 1),\n",
    "            C3(get_channels(512), get_channels(512), n_blocks=get_depth(9))\n",
    "        )\n",
    "        \n",
    "        self.stage4 = nn.Sequential(\n",
    "            Conv(get_channels(512), get_channels(1024), 3, 2, 1),\n",
    "            C3(get_channels(1024), get_channels(1024), n_blocks=get_depth(3)),\n",
    "            SPPF(get_channels(1024), get_channels(1024))\n",
    "        )\n",
    "        \n",
    "        self.out_channels = [\n",
    "            get_channels(128),   # P2\n",
    "            get_channels(256),   # P3\n",
    "            get_channels(512),   # P4\n",
    "            get_channels(1024)   # P5\n",
    "        ]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        p2 = self.stage1(x)\n",
    "        p3 = self.stage2(p2)\n",
    "        p4 = self.stage3(p3)\n",
    "        p5 = self.stage4(p4)\n",
    "        return [p2, p3, p4, p5]\n",
    "\n",
    "class SMRSELDWithCSPDarkNet(nn.Module):\n",
    "    \"\"\"\n",
    "    SMR‑SELD model with a CSPDarkNet53 backbone.\n",
    "\n",
    "    Input shape: [batch_size, 250, 4, 64] - [B, T, C, F]\n",
    "    Output shape: [batch_size, 250, I*J, num_classes] - [B, T, grid_cells, M]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_channels : int\n",
    "        Number of input channels (4 for FOA).\n",
    "    grid_size : tuple of int\n",
    "        (I, J) specifying number of elevation and azimuth bins (e.g., (18, 36) for 10° resolution).\n",
    "    num_classes : int\n",
    "        Number of event classes including background (14).\n",
    "    use_small : bool\n",
    "        If True, use a reduced backbone (depth and width multipliers).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels=4, grid_size=(18, 36), num_classes=14, use_small=True):\n",
    "        super().__init__()\n",
    "        self.I, self.J = grid_size\n",
    "        self.grid_cells = self.I * self.J\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # CSPDarkNet53 backbone\n",
    "        if use_small:\n",
    "            self.backbone = CSPDarkNet53(in_channels=n_channels, depth_multiple=0.33, width_multiple=0.5)\n",
    "        else:\n",
    "            self.backbone = CSPDarkNet53(in_channels=n_channels)\n",
    "\n",
    "        # Multi‑scale fusion\n",
    "        self.fusion = nn.ModuleList([\n",
    "            nn.Conv2d(self.backbone.out_channels[1], 256, 1),  # P3\n",
    "            nn.Conv2d(self.backbone.out_channels[2], 256, 1),  # P4\n",
    "            nn.Conv2d(self.backbone.out_channels[3], 256, 1),  # P5\n",
    "        ])\n",
    "\n",
    "        self.conv_fuse = nn.Sequential(\n",
    "            nn.Conv2d(256 * 3, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(512, 256, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        # Pooling layer that resamples feature maps to exactly `grid_cells` rows.\n",
    "        # We treat the frequency dimension as the vertical axis and collapse the\n",
    "        # singleton width dimension.  AdaptiveAvgPool2d with output size\n",
    "        # `(grid_cells, 1)` produces a feature vector for each grid cell.\n",
    "        self.grid_pool = nn.AdaptiveAvgPool2d((self.grid_cells, 1))\n",
    "\n",
    "        # Classifier head applied to each grid cell independently.  Input\n",
    "        # dimension matches the number of channels after fusion (256).\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of SMR-SELD model.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [B, T, C, F] = [batch_size, 250, 4, 64]\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape [B, T, grid_cells, num_classes] = [batch_size, 250, 648, 14]\n",
    "        \"\"\"\n",
    "        B, T, C, F_dim = x.shape  # [batch_size, 250, 4, 64]\n",
    "\n",
    "        # Reshape to [B*T, C, F, 1] for 2D CNN processing\n",
    "        # Treat each time frame independently, with frequency as spatial dimension\n",
    "        x = x.reshape(B * T, C, F_dim, 1)  # [B*T, 4, 64, 1]\n",
    "\n",
    "        # Pass through backbone - extracts multi-scale features\n",
    "        features = self.backbone(x)\n",
    "        p3, p4, p5 = features[1], features[2], features[3]\n",
    "\n",
    "        # Apply 1x1 convolutions for channel reduction\n",
    "        p3 = self.fusion[0](p3)  # [B*T, 256, H3, W3]\n",
    "        p4 = self.fusion[1](p4)  # [B*T, 256, H4, W4]\n",
    "        p5 = self.fusion[2](p5)  # [B*T, 256, H5, W5]\n",
    "\n",
    "        # Upsample all to same spatial size (align to p3)\n",
    "        target_size = p3.shape[2:]\n",
    "        p4 = F.interpolate(p4, size=target_size, mode='bilinear', align_corners=False)\n",
    "        p5 = F.interpolate(p5, size=target_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Concatenate multi-scale features\n",
    "        fused = torch.cat([p3, p4, p5], dim=1)  # [B*T, 768, H, W]\n",
    "        x = self.conv_fuse(fused)  # [B*T, 256, H, W]\n",
    "\n",
    "        # Resample feature maps to the number of grid cells.  Treat the\n",
    "        # frequency axis as the vertical dimension and the singleton width\n",
    "        # axis as horizontal.  This yields a tensor of shape\n",
    "        # [B*T, 256, grid_cells, 1].\n",
    "        x = self.grid_pool(x)  # [B*T, 256, grid_cells, 1]\n",
    "\n",
    "        # Remove the trailing width dimension and transpose to\n",
    "        # [B*T, grid_cells, 256] so each cell has its own feature vector.\n",
    "        x = x.squeeze(-1).permute(0, 2, 1)  # [B*T, G, 256]\n",
    "\n",
    "        # Apply the classifier independently to each cell.  The linear layer\n",
    "        # will broadcast over the grid dimension.  Output shape:\n",
    "        # [B*T, G, num_classes].\n",
    "        x = self.classifier(x)  # [B*T, G, M]\n",
    "\n",
    "        # Reshape back to [B, T, G, M]\n",
    "        x = x.view(B, T, self.grid_cells, self.num_classes)\n",
    "\n",
    "        # Apply softmax along the class dimension\n",
    "        return F.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77d310be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMRSELDLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete SMR‑SELD loss function with three components:\n",
    "    class MSE, AIUR and converging localization.  The grid dimensions (I, J)\n",
    "    are needed for the localization loss; they are derived from the\n",
    "    configuration at initialization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w_class=1.0, w_aiur=0.5, w_cl=0.5, grid_size=None):\n",
    "        super().__init__()\n",
    "        self.w_class = w_class\n",
    "        self.w_aiur = w_aiur\n",
    "        self.w_cl = w_cl\n",
    "        # grid_size should be a tuple (I, J).  If None, we assume square grid.\n",
    "        if grid_size is not None:\n",
    "            self.I, self.J = grid_size\n",
    "        else:\n",
    "            self.I = self.J = None\n",
    "    \n",
    "    def class_mse_loss(self, y_pred, y_true):\n",
    "        \"\"\"Class-wise Mean Squared Error loss with class weighting\n",
    "        \n",
    "        Apply higher weight to event classes (first 13) vs background (last).\n",
    "        This handles the severe class imbalance (~99.8% background).\n",
    "        \n",
    "        Args:\n",
    "            y_pred: Predicted output (B, T, G, M) where M = num_classes\n",
    "            y_true: Ground truth labels (B, T, G, M)\n",
    "        \n",
    "        Returns:\n",
    "            Weighted MSE loss (scalar tensor)\n",
    "        \"\"\"\n",
    "        # # Get dimensions\n",
    "        # B, T, G, M = y_pred.shape\n",
    "        \n",
    "        # # Background class is the last index (index 13 for 14 classes)\n",
    "        # background_idx = M - 1\n",
    "        \n",
    "        # # Create masks for event and background cells\n",
    "        # # A cell has an event if any non-background class has value 1\n",
    "        # # We check the true labels to determine which cells have events\n",
    "        # y_true_class = torch.argmax(y_true, dim=-1)  # (B, T, G) - get class index\n",
    "        \n",
    "        # # Mask for cells with events (non-background)\n",
    "        # event_mask = (y_true_class != background_idx)  # (B, T, G)\n",
    "        \n",
    "        # # Mask for background cells\n",
    "        # background_mask = (y_true_class == background_idx)  # (B, T, G)\n",
    "        \n",
    "        # # Compute squared error for all cells and all classes\n",
    "        # squared_error = (y_pred - y_true) ** 2  # (B, T, G, M)\n",
    "        \n",
    "        # # Sum across class dimension to get per-cell error\n",
    "        # squared_error_per_cell = squared_error.sum(dim=-1)  # (B, T, G)\n",
    "        \n",
    "        # # Separate losses for event cells and background cells\n",
    "        # if event_mask.sum() > 0:\n",
    "        #     event_loss = squared_error_per_cell[event_mask].mean()\n",
    "        # else:\n",
    "        #     event_loss = torch.tensor(0.0, device=y_pred.device)\n",
    "        \n",
    "        # if background_mask.sum() > 0:\n",
    "        #     background_loss = squared_error_per_cell[background_mask].mean()\n",
    "        # else:\n",
    "        #     background_loss = torch.tensor(0.0, device=y_pred.device)\n",
    "        \n",
    "        # # Apply weighted loss: higher weight for event classes to handle imbalance\n",
    "        # # Using 10:1 ratio as suggested in the original implementation\n",
    "        # weighted_loss = 10.0 * event_loss + 1.0 * background_loss\n",
    "        \n",
    "        # return weighted_loss\n",
    "        mse_loss = F.mse_loss(y_pred, y_true, reduction='mean')\n",
    "        return mse_loss\n",
    "        \n",
    "\n",
    "    \n",
    "    def aiur_loss(self, y_pred, y_true):\n",
    "        \"\"\"Area Intersection Union Ratio (AIUR) loss computed per frame and batch.\n",
    "\n",
    "        We compute the IoU for each frame in each sequence and average the results.\n",
    "        This encourages the model to improve IoU locally rather than aggregating\n",
    "        everything into a single global statistic, which can remain near 1 when\n",
    "        the model predicts only background.\n",
    "        \n",
    "        Args:\n",
    "            y_pred: Predicted output (B, T, G, M) where M = num_classes\n",
    "            y_true: Ground truth labels (B, T, G, M)\n",
    "        \n",
    "        Returns:\n",
    "            AIUR loss (scalar tensor): 1 - average_IoU across all frames and batches\n",
    "        \"\"\"\n",
    "        # Get dimensions\n",
    "        B, T, G, M = y_pred.shape\n",
    "        \n",
    "        # Background class is the last index (index 13 for 14 classes)\n",
    "        background_idx = M - 1\n",
    "        \n",
    "        # Get predicted and true class indices\n",
    "        y_pred_class = torch.argmax(y_pred, dim=-1)  # (B, T, G)\n",
    "        y_true_class = torch.argmax(y_true, dim=-1)  # (B, T, G)\n",
    "        \n",
    "        # Create binary masks: 1 for event cells, 0 for background\n",
    "        pred_event_mask = (y_pred_class != background_idx).float()  # (B, T, G)\n",
    "        true_event_mask = (y_true_class != background_idx).float()  # (B, T, G)\n",
    "        \n",
    "        # Compute IoU for each frame in each batch\n",
    "        # IoU = intersection / union\n",
    "        # intersection = cells that have events in BOTH pred and true\n",
    "        # union = cells that have events in EITHER pred or true\n",
    "        \n",
    "        # Intersection: element-wise multiplication\n",
    "        intersection = (pred_event_mask * true_event_mask).sum(dim=-1)  # (B, T) - sum over grid cells\n",
    "        \n",
    "        # Union: sum of both masks minus intersection\n",
    "        # Union = |A| + |B| - |A ∩ B|\n",
    "        pred_count = pred_event_mask.sum(dim=-1)  # (B, T)\n",
    "        true_count = true_event_mask.sum(dim=-1)  # (B, T)\n",
    "        union = pred_count + true_count - intersection  # (B, T)\n",
    "        \n",
    "        # Compute IoU for each frame\n",
    "        # Add small epsilon to avoid division by zero\n",
    "        epsilon = 1e-8\n",
    "        iou = intersection / (union + epsilon)  # (B, T)\n",
    "        \n",
    "        # Handle edge case where both pred and true have no events (union = 0)\n",
    "        # In this case, IoU should be 1.0 (perfect match of empty sets)\n",
    "        iou = torch.where(union > 0, iou, torch.ones_like(iou))\n",
    "        \n",
    "        # Average IoU across all frames and batches\n",
    "        avg_iou = iou.mean()\n",
    "        \n",
    "        # AIUR loss = 1 - IoU\n",
    "        aiur_loss_value = 1.0 - avg_iou\n",
    "        \n",
    "        return aiur_loss_value\n",
    "\n",
    "    \n",
    "    def converging_localization_loss(self, y_pred, y_true):\n",
    "        \"\"\"Converging Localization loss\n",
    "        \n",
    "        This loss encourages predictions to converge towards dense non-background areas.\n",
    "        It guides the prediction results from surrounding areas to the target location,\n",
    "        reducing extreme category imbalance and strengthening regression positioning ability.\n",
    "        \n",
    "        From the paper:\n",
    "        1. Transform targets: y'_ij = 1 for background cells, -N_bac/N_non_bac for event cells\n",
    "        2. Calculate attention: yat_ij = y'_ij + AVG(surroundings - y'_ij)\n",
    "        3. Loss: LCL = Σ(ŷ_ij × yat_ij) / total_cells\n",
    "        \n",
    "        Args:\n",
    "            y_pred: Predicted output (B, T, G, M) where M = num_classes\n",
    "            y_true: Ground truth labels (B, T, G, M)\n",
    "        \n",
    "        Returns:\n",
    "            Converging localization loss (scalar tensor)\n",
    "        \"\"\"\n",
    "        # Get dimensions\n",
    "        B, T, G, M = y_pred.shape\n",
    "        # Determine grid dimensions\n",
    "        if self.I is not None and self.J is not None:\n",
    "            I, J = self.I, self.J\n",
    "        else:\n",
    "            I = J = int(math.sqrt(G))\n",
    "        # Reshape to (B, T, I, J)\n",
    "        true_nonbg = y_true[..., :-1].sum(dim=-1).view(B, T, I, J)\n",
    "        pred_nonbg = y_pred[..., :-1].sum(dim=-1).view(B, T, I, J)\n",
    "        # Count per frame\n",
    "        N_bac = (true_nonbg < 0.01).sum(dim=(2, 3), keepdim=True).float()\n",
    "        N_non_bac = (true_nonbg > 0.01).sum(dim=(2, 3), keepdim=True).float()\n",
    "        # Initialise y_prime: 1 for background, negative ratio for events\n",
    "        y_prime = torch.ones_like(true_nonbg)\n",
    "        ratio = -(N_bac / (N_non_bac + 1e-10))\n",
    "        y_prime = torch.where(true_nonbg > 0.01, ratio.expand_as(true_nonbg), y_prime)\n",
    "        # Compute neighbourhood average using circular padding\n",
    "        y_prime_padded = F.pad(y_prime, (1, 1, 1, 1), mode='circular')  # pad (left,right,top,bottom)\n",
    "        diff_sum = torch.zeros_like(y_prime)\n",
    "        for di in (-1, 0, 1):\n",
    "            for dj in (-1, 0, 1):\n",
    "                if di == 0 and dj == 0:\n",
    "                    continue\n",
    "                neighbor = y_prime_padded[:, :, 1+di:I+1+di, 1+dj:J+1+dj]\n",
    "                diff_sum += (neighbor - y_prime)\n",
    "        avg_diff = diff_sum / 8.0\n",
    "        y_at = y_prime + avg_diff\n",
    "        # Mask frames without events\n",
    "        has_events_mask = (N_non_bac > 0).float()\n",
    "        loss = ((pred_nonbg * y_at) * has_events_mask).sum() / (B * T + 1e-10)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the weighted sum of the three loss components and return\n",
    "        both the total loss and a breakdown of individual components.  This\n",
    "        method is called when the loss module is invoked like a function,\n",
    "        e.g., `loss, breakdown = criterion(pred, target)`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : torch.Tensor\n",
    "            Predicted output of shape (B, T, G, M), where B is batch size,\n",
    "            T is the number of frames in a sequence, G is the number of\n",
    "            grid cells and M is the number of classes.\n",
    "        y_true : torch.Tensor\n",
    "            Ground‑truth targets with the same shape as ``y_pred``.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        total_loss : torch.Tensor\n",
    "            Scalar tensor containing the weighted sum of the three loss terms.\n",
    "        breakdown : dict\n",
    "            Dictionary with keys ``'class_mse'``, ``'aiur'`` and ``'cl'`` mapping\n",
    "            to the individual component losses (Python floats).\n",
    "        \"\"\"\n",
    "        # Compute individual losses\n",
    "        loss_class = self.class_mse_loss(y_pred, y_true)\n",
    "        loss_aiur = self.aiur_loss(y_pred, y_true)\n",
    "        loss_cl = self.converging_localization_loss(y_pred, y_true)\n",
    "        # Weighted sum\n",
    "        total_loss = (self.w_class * loss_class +\n",
    "                      self.w_aiur * loss_aiur +\n",
    "                      self.w_cl * loss_cl)\n",
    "        # Prepare breakdown for logging\n",
    "        breakdown = {\n",
    "            'class_mse': float(loss_class.item()),\n",
    "            'aiur': float(loss_aiur.item()),\n",
    "            'cl': float(loss_cl.item())\n",
    "        }\n",
    "        return total_loss, breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ae2d081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train dataset...\n",
      "2025-11-16 23:43:43 - SMR_SELD - INFO - SELDDataset initialization started...\n",
      "2025-11-16 23:43:43 - SMR_SELD - INFO -   Files: 1 audio files\n",
      "2025-11-16 23:43:43 - SMR_SELD - INFO -   Grid: 18x36 = 648 cells\n",
      "2025-11-16 23:43:43 - SMR_SELD - INFO -   Window: 250 frames (5.0s)\n",
      "2025-11-16 23:43:43 - SMR_SELD - INFO -   Hop: 50 frames (1.0s)\n",
      "2025-11-16 23:43:43 - SMR_SELD - INFO - Loading and processing all audio files...\n",
      "2025-11-16 23:43:43 - SMR_SELD - INFO -   Files: 1 audio files\n",
      "2025-11-16 23:43:43 - SMR_SELD - INFO -   Grid: 18x36 = 648 cells\n",
      "2025-11-16 23:43:43 - SMR_SELD - INFO -   Window: 250 frames (5.0s)\n",
      "2025-11-16 23:43:43 - SMR_SELD - INFO -   Hop: 50 frames (1.0s)\n",
      "2025-11-16 23:43:43 - SMR_SELD - INFO - Loading and processing all audio files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 1/1 [00:18<00:00, 18.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-16 23:44:01 - SMR_SELD - INFO - Concatenated data: 4470 total frames\n",
      "2025-11-16 23:44:01 - SMR_SELD - INFO -   Spectrograms shape: torch.Size([4, 64, 4470])\n",
      "2025-11-16 23:44:01 - SMR_SELD - INFO -   Labels shape: torch.Size([4470, 648, 14])\n",
      "2025-11-16 23:44:01 - SMR_SELD - INFO -   Spectrograms shape: torch.Size([4, 64, 4470])\n",
      "2025-11-16 23:44:01 - SMR_SELD - INFO -   Labels shape: torch.Size([4470, 648, 14])\n",
      "2025-11-16 23:44:01 - SMR_SELD - INFO - Created 90 windows\n",
      "2025-11-16 23:44:01 - SMR_SELD - INFO - SELDDataset initialized with 90 windows\n",
      "\n",
      "Creating test dataset...\n",
      "2025-11-16 23:44:01 - SMR_SELD - INFO - SELDDataset initialization started...\n",
      "2025-11-16 23:44:01 - SMR_SELD - INFO -   Files: 1 audio files\n",
      "2025-11-16 23:44:01 - SMR_SELD - INFO -   Grid: 18x36 = 648 cells\n",
      "2025-11-16 23:44:01 - SMR_SELD - INFO -   Window: 250 frames (5.0s)\n",
      "2025-11-16 23:44:01 - SMR_SELD - INFO -   Hop: 50 frames (1.0s)\n",
      "2025-11-16 23:44:01 - SMR_SELD - INFO - Loading and processing all audio files...\n",
      "2025-11-16 23:44:01 - SMR_SELD - INFO - Created 90 windows\n",
      "2025-11-16 23:44:01 - SMR_SELD - INFO - SELDDataset initialized with 90 windows\n",
      "\n",
      "Creating test dataset...\n",
      "2025-11-16 23:44:01 - SMR_SELD - INFO - SELDDataset initialization started...\n",
      "2025-11-16 23:44:01 - SMR_SELD - INFO -   Files: 1 audio files\n",
      "2025-11-16 23:44:01 - SMR_SELD - INFO -   Grid: 18x36 = 648 cells\n",
      "2025-11-16 23:44:01 - SMR_SELD - INFO -   Window: 250 frames (5.0s)\n",
      "2025-11-16 23:44:01 - SMR_SELD - INFO -   Hop: 50 frames (1.0s)\n",
      "2025-11-16 23:44:01 - SMR_SELD - INFO - Loading and processing all audio files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing files: 100%|██████████| 1/1 [00:12<00:00, 12.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-16 23:44:14 - SMR_SELD - INFO - Concatenated data: 3035 total frames\n",
      "2025-11-16 23:44:14 - SMR_SELD - INFO -   Spectrograms shape: torch.Size([4, 64, 3035])\n",
      "2025-11-16 23:44:14 - SMR_SELD - INFO -   Labels shape: torch.Size([3035, 648, 14])\n",
      "2025-11-16 23:44:14 - SMR_SELD - INFO -   Spectrograms shape: torch.Size([4, 64, 3035])\n",
      "2025-11-16 23:44:14 - SMR_SELD - INFO -   Labels shape: torch.Size([3035, 648, 14])\n",
      "2025-11-16 23:44:14 - SMR_SELD - INFO - Created 61 windows\n",
      "2025-11-16 23:44:14 - SMR_SELD - INFO - SELDDataset initialized with 61 windows\n",
      "\n",
      "============================================================\n",
      "Dataset Summary:\n",
      "============================================================\n",
      "Train dataset: 90 windows\n",
      "Test dataset: 61 windows\n",
      "Grid dimensions: 18x36 = 648 cells\n",
      "Window length: 250 frames (5.0s)\n",
      "Hop length: 50 frames (1.0s)\n",
      "2025-11-16 23:44:14 - SMR_SELD - INFO - Created 61 windows\n",
      "2025-11-16 23:44:14 - SMR_SELD - INFO - SELDDataset initialized with 61 windows\n",
      "\n",
      "============================================================\n",
      "Dataset Summary:\n",
      "============================================================\n",
      "Train dataset: 90 windows\n",
      "Test dataset: 61 windows\n",
      "Grid dimensions: 18x36 = 648 cells\n",
      "Window length: 250 frames (5.0s)\n",
      "Hop length: 50 frames (1.0s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create train and test datasets\n",
    "print(\"Creating train dataset...\")\n",
    "train_dataset = SELDDataset(\n",
    "    audio_files=train_audio_files[:1],\n",
    "    metadata_files=train_meta_files[:1],\n",
    "    num_classes=14\n",
    ")\n",
    "\n",
    "print(\"\\nCreating test dataset...\")\n",
    "test_dataset = SELDDataset(\n",
    "    audio_files=test_audio_files[:1],\n",
    "    metadata_files=test_meta_files[:1],\n",
    "    num_classes=14\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Dataset Summary:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Train dataset: {len(train_dataset)} windows\")\n",
    "print(f\"Test dataset: {len(test_dataset)} windows\")\n",
    "print(f\"Grid dimensions: {train_dataset.I}x{train_dataset.J} = {train_dataset.total_cells} cells\")\n",
    "print(f\"Window length: {train_dataset.window_length_frames} frames ({train_dataset.window_length_samples / train_dataset.sample_rate:.1f}s)\")\n",
    "print(f\"Hop length: {train_dataset.hop_length_frames} frames ({train_dataset.hop_length_samples / train_dataset.sample_rate:.1f}s)\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e608490",
   "metadata": {},
   "source": [
    "### Model Testing - Single Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10f2516b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL FORWARD PASS TEST\n",
      "============================================================\n",
      "\n",
      "Model initialized and moved to cuda\n",
      "Model parameters: 30,744,846\n",
      "\n",
      "------------------------------------------------------------\n",
      "Getting a single window from dataset...\n",
      "------------------------------------------------------------\n",
      "Spectrogram shape: torch.Size([250, 4, 64])\n",
      "Labels shape: torch.Size([250, 648, 14])\n",
      "\n",
      "Batch spectrogram shape: torch.Size([1, 250, 4, 64])\n",
      "Batch labels shape: torch.Size([1, 250, 648, 14])\n",
      "\n",
      "------------------------------------------------------------\n",
      "Running forward pass...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Model initialized and moved to cuda\n",
      "Model parameters: 30,744,846\n",
      "\n",
      "------------------------------------------------------------\n",
      "Getting a single window from dataset...\n",
      "------------------------------------------------------------\n",
      "Spectrogram shape: torch.Size([250, 4, 64])\n",
      "Labels shape: torch.Size([250, 648, 14])\n",
      "\n",
      "Batch spectrogram shape: torch.Size([1, 250, 4, 64])\n",
      "Batch labels shape: torch.Size([1, 250, 648, 14])\n",
      "\n",
      "------------------------------------------------------------\n",
      "Running forward pass...\n",
      "------------------------------------------------------------\n",
      "Predictions shape: torch.Size([1, 250, 648, 14])\n",
      "\n",
      "------------------------------------------------------------\n",
      "DIMENSION VERIFICATION\n",
      "------------------------------------------------------------\n",
      "✓ Input shape:       torch.Size([1, 250, 4, 64]) -> [batch_size, T, C, F]\n",
      "✓ Output shape:      torch.Size([1, 250, 648, 14]) -> [batch_size, T, grid_cells, num_classes]\n",
      "✓ Ground truth shape: torch.Size([1, 250, 648, 14]) -> [batch_size, T, grid_cells, num_classes]\n",
      "✓ Shapes match: True\n",
      "\n",
      "------------------------------------------------------------\n",
      "PREDICTIONS vs GROUND TRUTH COMPARISON\n",
      "------------------------------------------------------------\n",
      "\n",
      "Time frame 0, showing first 5 cells:\n",
      "Cell   Pred Class   True Class   Pred Prob    Match \n",
      "------------------------------------------------------------\n",
      "0      13           13           0.0771       ✓     \n",
      "1      13           13           0.0771       ✓     \n",
      "2      13           13           0.0771       ✓     \n",
      "3      13           13           0.0771       ✓     \n",
      "4      13           13           0.0771       ✓     \n",
      "\n",
      "------------------------------------------------------------\n",
      "SUMMARY STATISTICS\n",
      "------------------------------------------------------------\n",
      "Overall accuracy: 99.50%\n",
      "\n",
      "Class distribution:\n",
      "Class                Predicted    Ground Truth\n",
      "--------------------------------------------------\n",
      "Female speech, wom.. 0            0           \n",
      "Male speech, man s.. 0            280         \n",
      "Clapping             0            0           \n",
      "Telephone            0            0           \n",
      "Laughter             0            0           \n",
      "Domestic sounds      0            245         \n",
      "Walk, footsteps      0            45          \n",
      "Door, open or close  0            0           \n",
      "Music                0            245         \n",
      "Musical instrument   0            0           \n",
      "Water tap, faucet    0            0           \n",
      "Bell                 0            0           \n",
      "Knock                0            0           \n",
      "Background           162000       161185      \n",
      "\n",
      "============================================================\n",
      "MODEL TEST COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      "Note: Low accuracy is expected for untrained model (random initialization).\n",
      "Predictions shape: torch.Size([1, 250, 648, 14])\n",
      "\n",
      "------------------------------------------------------------\n",
      "DIMENSION VERIFICATION\n",
      "------------------------------------------------------------\n",
      "✓ Input shape:       torch.Size([1, 250, 4, 64]) -> [batch_size, T, C, F]\n",
      "✓ Output shape:      torch.Size([1, 250, 648, 14]) -> [batch_size, T, grid_cells, num_classes]\n",
      "✓ Ground truth shape: torch.Size([1, 250, 648, 14]) -> [batch_size, T, grid_cells, num_classes]\n",
      "✓ Shapes match: True\n",
      "\n",
      "------------------------------------------------------------\n",
      "PREDICTIONS vs GROUND TRUTH COMPARISON\n",
      "------------------------------------------------------------\n",
      "\n",
      "Time frame 0, showing first 5 cells:\n",
      "Cell   Pred Class   True Class   Pred Prob    Match \n",
      "------------------------------------------------------------\n",
      "0      13           13           0.0771       ✓     \n",
      "1      13           13           0.0771       ✓     \n",
      "2      13           13           0.0771       ✓     \n",
      "3      13           13           0.0771       ✓     \n",
      "4      13           13           0.0771       ✓     \n",
      "\n",
      "------------------------------------------------------------\n",
      "SUMMARY STATISTICS\n",
      "------------------------------------------------------------\n",
      "Overall accuracy: 99.50%\n",
      "\n",
      "Class distribution:\n",
      "Class                Predicted    Ground Truth\n",
      "--------------------------------------------------\n",
      "Female speech, wom.. 0            0           \n",
      "Male speech, man s.. 0            280         \n",
      "Clapping             0            0           \n",
      "Telephone            0            0           \n",
      "Laughter             0            0           \n",
      "Domestic sounds      0            245         \n",
      "Walk, footsteps      0            45          \n",
      "Door, open or close  0            0           \n",
      "Music                0            245         \n",
      "Musical instrument   0            0           \n",
      "Water tap, faucet    0            0           \n",
      "Bell                 0            0           \n",
      "Knock                0            0           \n",
      "Background           162000       161185      \n",
      "\n",
      "============================================================\n",
      "MODEL TEST COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      "Note: Low accuracy is expected for untrained model (random initialization).\n"
     ]
    }
   ],
   "source": [
    "# Test model with a single window\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL FORWARD PASS TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize the model\n",
    "model = SMRSELDWithCSPDarkNet(\n",
    "    n_channels=4,\n",
    "    grid_size=(18, 36),  # 18x36 = 648 cells\n",
    "    num_classes=14,\n",
    "    use_small=False\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(DEVICE)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"\\nModel initialized and moved to {DEVICE}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Get a single window from test dataset\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Getting a single window from dataset...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "spec, labels = test_dataset[0]\n",
    "print(f\"Spectrogram shape: {spec.shape}\")  # Expected: (250, 4, 64)\n",
    "print(f\"Labels shape: {labels.shape}\")      # Expected: (250, 648, 14)\n",
    "\n",
    "# Add batch dimension and move to device\n",
    "spec_batch = spec.unsqueeze(0).to(DEVICE)      # Shape: (1, 250, 4, 64)\n",
    "labels_batch = labels.unsqueeze(0).to(DEVICE)  # Shape: (1, 250, 648, 14)\n",
    "\n",
    "print(f\"\\nBatch spectrogram shape: {spec_batch.shape}\")  # (1, 250, 4, 64)\n",
    "print(f\"Batch labels shape: {labels_batch.shape}\")      # (1, 250, 648, 14)\n",
    "\n",
    "# Forward pass\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Running forward pass...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(spec_batch)\n",
    "\n",
    "print(f\"Predictions shape: {predictions.shape}\")  # Expected: (1, 250, 648, 14)\n",
    "\n",
    "# Verify dimensions match\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"DIMENSION VERIFICATION\")\n",
    "print(\"-\"*60)\n",
    "print(f\"✓ Input shape:       {spec_batch.shape} -> [batch_size, T, C, F]\")\n",
    "print(f\"✓ Output shape:      {predictions.shape} -> [batch_size, T, grid_cells, num_classes]\")\n",
    "print(f\"✓ Ground truth shape: {labels_batch.shape} -> [batch_size, T, grid_cells, num_classes]\")\n",
    "print(f\"✓ Shapes match: {predictions.shape == labels_batch.shape}\")\n",
    "\n",
    "# Compare predictions with ground truth\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"PREDICTIONS vs GROUND TRUTH COMPARISON\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Get predictions for first time frame, first 5 cells\n",
    "time_frame = 0\n",
    "num_cells_to_show = 5\n",
    "\n",
    "print(f\"\\nTime frame {time_frame}, showing first {num_cells_to_show} cells:\")\n",
    "print(f\"{'Cell':<6} {'Pred Class':<12} {'True Class':<12} {'Pred Prob':<12} {'Match':<6}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for cell_idx in range(num_cells_to_show):\n",
    "    pred_class = torch.argmax(predictions[0, time_frame, cell_idx]).item()\n",
    "    true_class = torch.argmax(labels_batch[0, time_frame, cell_idx]).item()\n",
    "    pred_prob = predictions[0, time_frame, cell_idx, pred_class].item()\n",
    "    match = \"✓\" if pred_class == true_class else \"✗\"\n",
    "    \n",
    "    print(f\"{cell_idx:<6} {pred_class:<12} {true_class:<12} {pred_prob:<12.4f} {match:<6}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Get predicted and true classes for all frames and cells\n",
    "pred_classes = torch.argmax(predictions, dim=-1)  # Shape: (1, 250, 648)\n",
    "true_classes = torch.argmax(labels_batch, dim=-1)  # Shape: (1, 250, 648)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (pred_classes == true_classes).float().mean().item()\n",
    "print(f\"Overall accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# Count predictions per class\n",
    "pred_class_counts = torch.bincount(pred_classes.flatten(), minlength=14)\n",
    "true_class_counts = torch.bincount(true_classes.flatten(), minlength=14)\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"{'Class':<20} {'Predicted':<12} {'Ground Truth':<12}\")\n",
    "print(\"-\" * 50)\n",
    "for class_idx in range(14):\n",
    "    class_name = config.STARSS22_CLASSES.get(class_idx, f\"Class {class_idx}\")\n",
    "    # Truncate long names\n",
    "    class_name = class_name[:18] + \"..\" if len(class_name) > 20 else class_name\n",
    "    print(f\"{class_name:<20} {pred_class_counts[class_idx].item():<12} {true_class_counts[class_idx].item():<12}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL TEST COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNote: Low accuracy is expected for untrained model (random initialization).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b6863fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(train_losses, test_losses, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot and save training and test loss curves.\n",
    "    \n",
    "    Args:\n",
    "        train_losses: List of training losses per epoch\n",
    "        test_losses: List of test losses per epoch\n",
    "        save_path: Path to save the plot (optional)\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.plot(epochs, train_losses, 'b-o', label='Training Loss', linewidth=2, markersize=4)\n",
    "    plt.plot(epochs, test_losses, 'r-s', label='Test Loss', linewidth=2, markersize=4)\n",
    "    \n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title('Training and Test Loss Curves', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add minimum loss markers\n",
    "    min_train_idx = train_losses.index(min(train_losses))\n",
    "    min_test_idx = test_losses.index(min(test_losses))\n",
    "    \n",
    "    plt.plot(min_train_idx + 1, train_losses[min_train_idx], 'b*', \n",
    "             markersize=15, label=f'Best Train: {train_losses[min_train_idx]:.4f}')\n",
    "    plt.plot(min_test_idx + 1, test_losses[min_test_idx], 'r*', \n",
    "             markersize=15, label=f'Best Test: {test_losses[min_test_idx]:.4f}')\n",
    "    \n",
    "    plt.legend(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        logger.info(f\"Loss curve saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180be89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    train_audio_files,\n",
    "    train_meta_files,\n",
    "    test_audio_files,\n",
    "    test_meta_files,\n",
    "    num_epochs=None,\n",
    "    batch_size=None,\n",
    "    learning_rate=None,\n",
    "    device=None,\n",
    "    use_small_model=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete training function for SMR-SELD model.\n",
    "    \n",
    "    Args:\n",
    "        train_audio_files: List of training audio file paths\n",
    "        train_meta_files: List of training metadata file paths\n",
    "        test_audio_files: List of test audio file paths\n",
    "        test_meta_files: List of test metadata file paths\n",
    "        num_epochs: Number of training epochs (default: from config)\n",
    "        batch_size: Batch size (default: from config)\n",
    "        learning_rate: Initial learning rate (default: from config)\n",
    "        device: Device to train on (default: DEVICE global)\n",
    "        use_small_model: Whether to use small backbone (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        history: Dictionary with training history\n",
    "    \"\"\"\n",
    "    # Use config defaults if not provided\n",
    "    num_epochs = config.NUM_EPOCHS\n",
    "    batch_size = config.BATCH_SIZE\n",
    "    learning_rate = config.LEARNING_RATE\n",
    "    device = DEVICE\n",
    "    \n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(\"STARTING TRAINING\")\n",
    "    logger.info(\"=\"*80)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 1: Load datasets\n",
    "    # ========================================================================\n",
    "    logger.info(\"\\nStep 1: Loading datasets...\")\n",
    "    \n",
    "    train_dataset = SELDDataset(\n",
    "        audio_files=train_audio_files,\n",
    "        metadata_files=train_meta_files,\n",
    "        num_classes=config.NUM_CLASSES\n",
    "    )\n",
    "    \n",
    "    test_dataset = SELDDataset(\n",
    "        audio_files=test_audio_files,\n",
    "        metadata_files=test_meta_files,\n",
    "        num_classes=config.NUM_CLASSES\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Set to 0 for Windows compatibility\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Train dataset: {len(train_dataset)} windows ({len(train_loader)} batches)\")\n",
    "    logger.info(f\"Test dataset: {len(test_dataset)} windows ({len(test_loader)} batches)\")\n",
    "    logger.info(f\"Grid dimensions: {train_dataset.I}x{train_dataset.J} = {train_dataset.total_cells} cells\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 2: Initialize model, loss, optimizer\n",
    "    # ========================================================================\n",
    "    logger.info(\"\\nStep 2: Initializing model, loss, and optimizer...\")\n",
    "    \n",
    "    model = SMRSELDWithCSPDarkNet(\n",
    "        n_channels=config.N_CHANNELS,\n",
    "        grid_size=(train_dataset.I, train_dataset.J),\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        use_small=use_small_model\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = SMRSELDLoss(\n",
    "        w_class=config.W_CLASS,\n",
    "        w_aiur=config.W_AIUR,\n",
    "        w_cl=config.W_CL,\n",
    "        grid_size=(train_dataset.I, train_dataset.J)\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=config.WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=config.LR_DECAY_FACTOR,\n",
    "        patience=config.LR_DECAY_PATIENCE,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    logger.info(f\"Optimizer: Adam (lr={learning_rate}, weight_decay={config.WEIGHT_DECAY})\")\n",
    "    logger.info(f\"Scheduler: ReduceLROnPlateau (factor={config.LR_DECAY_FACTOR}, patience={config.LR_DECAY_PATIENCE})\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 3: Training setup\n",
    "    # ========================================================================\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    epochs_without_improvement = 0\n",
    "    checkpoint_files = []\n",
    "    \n",
    "    logger.info(f\"\\nTraining configuration:\")\n",
    "    logger.info(f\"  Epochs: {num_epochs}\")\n",
    "    logger.info(f\"  Batch size: {batch_size}\")\n",
    "    logger.info(f\"  Learning rate: {learning_rate}\")\n",
    "    logger.info(f\"  Early stopping patience: {config.PATIENCE}\")\n",
    "    logger.info(f\"  Min delta: {config.MIN_DELTA}\")\n",
    "    logger.info(f\"  Save every N epochs: {config.SAVE_EVERY_N_EPOCHS}\")\n",
    "    logger.info(f\"  Device: {device}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 4: Training loop\n",
    "    # ========================================================================\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"STARTING TRAINING LOOP\")\n",
    "    logger.info(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_start_time = datetime.now()\n",
    "        \n",
    "        # ====================================================================\n",
    "        # Training phase\n",
    "        # ====================================================================\n",
    "        model.train()\n",
    "        train_loss_accum = 0.0\n",
    "        train_class_mse_accum = 0.0\n",
    "        train_aiur_accum = 0.0\n",
    "        train_cl_accum = 0.0\n",
    "        \n",
    "        train_progress = tqdm(\n",
    "            train_loader,\n",
    "            desc=f\"Epoch {epoch}/{num_epochs} [Train]\",\n",
    "            leave=False\n",
    "        )\n",
    "        \n",
    "        for batch_idx, (spectrograms, labels) in enumerate(train_progress):\n",
    "            # Move to device\n",
    "            spectrograms = spectrograms.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(spectrograms)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss, breakdown = criterion(predictions, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate losses\n",
    "            train_loss_accum += loss.item()\n",
    "            train_class_mse_accum += breakdown['class_mse']\n",
    "            train_aiur_accum += breakdown['aiur']\n",
    "            train_cl_accum += breakdown['cl']\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_progress.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'lr': f\"{optimizer.param_groups[0]['lr']:.6f}\"\n",
    "            })\n",
    "        \n",
    "        # Average training losses\n",
    "        avg_train_loss = train_loss_accum / len(train_loader)\n",
    "        avg_train_class_mse = train_class_mse_accum / len(train_loader)\n",
    "        avg_train_aiur = train_aiur_accum / len(train_loader)\n",
    "        avg_train_cl = train_cl_accum / len(train_loader)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # Validation phase\n",
    "        # ====================================================================\n",
    "        model.eval()\n",
    "        test_loss_accum = 0.0\n",
    "        test_class_mse_accum = 0.0\n",
    "        test_aiur_accum = 0.0\n",
    "        test_cl_accum = 0.0\n",
    "        \n",
    "        test_progress = tqdm(\n",
    "            test_loader,\n",
    "            desc=f\"Epoch {epoch}/{num_epochs} [Test]\",\n",
    "            leave=False\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for spectrograms, labels in test_progress:\n",
    "                # Move to device\n",
    "                spectrograms = spectrograms.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                predictions = model(spectrograms)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss, breakdown = criterion(predictions, labels)\n",
    "                \n",
    "                # Accumulate losses\n",
    "                test_loss_accum += loss.item()\n",
    "                test_class_mse_accum += breakdown['class_mse']\n",
    "                test_aiur_accum += breakdown['aiur']\n",
    "                test_cl_accum += breakdown['cl']\n",
    "                \n",
    "                # Update progress bar\n",
    "                test_progress.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        # Average test losses\n",
    "        avg_test_loss = test_loss_accum / len(test_loader)\n",
    "        avg_test_class_mse = test_class_mse_accum / len(test_loader)\n",
    "        avg_test_aiur = test_aiur_accum / len(test_loader)\n",
    "        avg_test_cl = test_cl_accum / len(test_loader)\n",
    "        \n",
    "        # Store losses\n",
    "        train_losses.append(avg_train_loss)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(avg_test_loss)\n",
    "        \n",
    "        # Calculate epoch duration\n",
    "        epoch_duration = (datetime.now() - epoch_start_time).total_seconds()\n",
    "        \n",
    "        # ====================================================================\n",
    "        # Logging\n",
    "        # ====================================================================\n",
    "        logger.info(f\"\\nEpoch {epoch}/{num_epochs} - Duration: {epoch_duration:.1f}s\")\n",
    "        logger.info(f\"  Train Loss: {avg_train_loss:.6f} (MSE: {avg_train_class_mse:.6f}, AIUR: {avg_train_aiur:.6f}, CL: {avg_train_cl:.6f})\")\n",
    "        logger.info(f\"  Test Loss:  {avg_test_loss:.6f} (MSE: {avg_test_class_mse:.6f}, AIUR: {avg_test_aiur:.6f}, CL: {avg_test_cl:.6f})\")\n",
    "        logger.info(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # Save best model\n",
    "        # ====================================================================\n",
    "        if avg_test_loss < best_test_loss - config.MIN_DELTA:\n",
    "            improvement = best_test_loss - avg_test_loss\n",
    "            best_test_loss = avg_test_loss\n",
    "            best_epoch = epoch\n",
    "            epochs_without_improvement = 0\n",
    "            \n",
    "            best_model_path = config.CHECKPOINT_PATH / \"best_model.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'test_loss': avg_test_loss,\n",
    "                'config': config\n",
    "            }, best_model_path)\n",
    "            \n",
    "            logger.info(f\"  New best model saved! (improvement: {improvement:.6f})\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            logger.info(f\"  No improvement for {epochs_without_improvement} epoch(s)\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # Save periodic checkpoints\n",
    "        # ====================================================================\n",
    "        if epoch % config.SAVE_EVERY_N_EPOCHS == 0:\n",
    "            checkpoint_path = config.CHECKPOINT_PATH / f\"checkpoint_epoch_{epoch}.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'test_loss': avg_test_loss,\n",
    "                'config': config\n",
    "            }, checkpoint_path)\n",
    "            \n",
    "            checkpoint_files.append(checkpoint_path)\n",
    "            logger.info(f\"  Checkpoint saved: {checkpoint_path.name}\")\n",
    "            \n",
    "            # Keep only last N checkpoints\n",
    "            if len(checkpoint_files) > config.KEEP_LAST_N_CHECKPOINTS:\n",
    "                old_checkpoint = checkpoint_files.pop(0)\n",
    "                if old_checkpoint.exists():\n",
    "                    old_checkpoint.unlink()\n",
    "                    logger.info(f\"  Removed old checkpoint: {old_checkpoint.name}\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # Early stopping check\n",
    "        # ====================================================================\n",
    "        if epochs_without_improvement >= config.PATIENCE:\n",
    "            logger.info(f\"\\n{'='*80}\")\n",
    "            logger.info(f\"EARLY STOPPING at epoch {epoch}\")\n",
    "            logger.info(f\"No improvement for {config.PATIENCE} consecutive epochs\")\n",
    "            logger.info(f\"Best test loss: {best_test_loss:.6f} at epoch {best_epoch}\")\n",
    "            logger.info(f\"{'='*80}\\n\")\n",
    "            break\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 5: Training complete\n",
    "    # ========================================================================\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"TRAINING COMPLETE\")\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(f\"Total epochs trained: {epoch}\")\n",
    "    logger.info(f\"Best test loss: {best_test_loss:.6f} at epoch {best_epoch}\")\n",
    "    logger.info(f\"Final train loss: {train_losses[-1]:.6f}\")\n",
    "    logger.info(f\"Final test loss: {test_losses[-1]:.6f}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 6: Plot and save loss curves\n",
    "    # ========================================================================\n",
    "    logger.info(\"\\nGenerating loss curves...\")\n",
    "    loss_curve_path = config.OUTPUT_PATH / f\"loss_curves_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "    plot_loss_curves(train_losses, test_losses, save_path=loss_curve_path)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 7: Load best model weights\n",
    "    # ========================================================================\n",
    "    logger.info(\"\\nLoading best model weights...\")\n",
    "    best_checkpoint = torch.load(config.CHECKPOINT_PATH / \"best_model.pth\")\n",
    "    model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "    logger.info(f\"Best model loaded from epoch {best_checkpoint['epoch']}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Save training history\n",
    "    # ========================================================================\n",
    "    history = {\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses,\n",
    "        'best_test_loss': best_test_loss,\n",
    "        'best_epoch': best_epoch,\n",
    "        'total_epochs': epoch,\n",
    "        'config': {\n",
    "            'num_epochs': num_epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'learning_rate': learning_rate,\n",
    "            'grid_size': (train_dataset.I, train_dataset.J)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save history to file\n",
    "    history_path = config.OUTPUT_PATH / f\"training_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pth\"\n",
    "    torch.save(history, history_path)\n",
    "    logger.info(f\"Training history saved to {history_path}\")\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"ALL DONE!\")\n",
    "    logger.info(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de3539",
   "metadata": {},
   "source": [
    "## Training Pipeline Summary\n",
    "\n",
    "The complete training pipeline has been implemented with the following features:\n",
    "\n",
    "### 1. **Loss Function (`SMRSELDLoss`)**\n",
    "   - **Class MSE Loss**: Weighted MSE with 10:1 ratio for event:background classes to handle class imbalance\n",
    "   - **AIUR Loss**: Area Intersection Union Ratio computed per sample to encourage better spatial predictions\n",
    "   - **Converging Localization Loss**: Encourages predictions to converge towards dense non-background areas\n",
    "\n",
    "### 2. **Training Function (`train_model`)**\n",
    "   - ✅ Loads train and test datasets automatically\n",
    "   - ✅ Trains model with hyperparameters from Config class\n",
    "   - ✅ Computes both train and test losses each epoch\n",
    "   - ✅ Saves best model weights based on test loss\n",
    "   - ✅ Saves checkpoints every N epochs (configurable)\n",
    "   - ✅ Tracks losses for every epoch\n",
    "   - ✅ Plots and saves loss curves as images\n",
    "   - ✅ Implements early stopping with configurable patience\n",
    "   - ✅ Uses ReduceLROnPlateau scheduler for learning rate decay\n",
    "\n",
    "### 3. **Key Features**\n",
    "   - Progress bars with `tqdm` for training and validation\n",
    "   - Detailed logging for every epoch\n",
    "   - Automatic cleanup of old checkpoints (keeps last N)\n",
    "   - Saves training history to disk\n",
    "   - Returns trained model and history dictionary\n",
    "\n",
    "### 4. **Configuration Parameters (in Config class)**\n",
    "```python\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-3\n",
    "LR_DECAY_FACTOR = 0.5\n",
    "LR_DECAY_PATIENCE = 5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "W_CLASS = 1.0  # Class MSE weight\n",
    "W_AIUR = 0.5   # AIUR loss weight\n",
    "W_CL = 0.5     # Converging localization weight\n",
    "PATIENCE = 10  # Early stopping patience\n",
    "MIN_DELTA = 1e-4  # Minimum improvement threshold\n",
    "SAVE_EVERY_N_EPOCHS = 5\n",
    "KEEP_LAST_N_CHECKPOINTS = 3\n",
    "```\n",
    "\n",
    "### 5. **Outputs**\n",
    "   - `best_model.pth`: Best performing model weights\n",
    "   - `checkpoint_epoch_N.pth`: Periodic checkpoints\n",
    "   - `loss_curves_TIMESTAMP.png`: Training/test loss plot\n",
    "   - `training_history_TIMESTAMP.pth`: Complete training history\n",
    "\n",
    "### 6. **Usage**\n",
    "Simply uncomment and run the training cell below to start training with all default parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f6401",
   "metadata": {},
   "source": [
    "### Model Testing and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efa53ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_grid_predictions(\n",
    "    ground_truth,\n",
    "    predictions,\n",
    "    time_frame,\n",
    "    grid_size,\n",
    "    title_prefix=\"\",\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize ground truth and predictions on a 2D grid for a specific time frame.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth: Ground truth tensor of shape (grid_cells, num_classes)\n",
    "        predictions: Predictions tensor of shape (grid_cells, num_classes)\n",
    "        time_frame: Time frame index being visualized\n",
    "        grid_size: Tuple (I, J) for grid dimensions\n",
    "        title_prefix: Prefix for plot titles\n",
    "        save_path: Path to save the figure (optional)\n",
    "    \n",
    "    Returns:\n",
    "        fig: Matplotlib figure object\n",
    "    \"\"\"\n",
    "    I, J = grid_size\n",
    "    \n",
    "    # Get class predictions (argmax)\n",
    "    gt_classes = torch.argmax(ground_truth, dim=-1).cpu().numpy()  # (grid_cells,)\n",
    "    pred_classes = torch.argmax(predictions, dim=-1).cpu().numpy()  # (grid_cells,)\n",
    "    \n",
    "    # Reshape to 2D grid\n",
    "    gt_grid = gt_classes.reshape(I, J)\n",
    "    pred_grid = pred_classes.reshape(I, J)\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Define colormap (background = 13 will be white/light)\n",
    "    cmap = plt.cm.get_cmap('tab20', 14)\n",
    "    \n",
    "    # Plot ground truth\n",
    "    im1 = axes[0].imshow(gt_grid, cmap=cmap, vmin=0, vmax=13, aspect='auto')\n",
    "    axes[0].set_title(f'{title_prefix}Ground Truth\\nFrame {time_frame}', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Azimuth bins (J)', fontsize=11)\n",
    "    axes[0].set_ylabel('Elevation bins (I)', fontsize=11)\n",
    "    axes[0].grid(True, alpha=0.3, color='gray', linewidth=0.5)\n",
    "    \n",
    "    # Plot predictions\n",
    "    im2 = axes[1].imshow(pred_grid, cmap=cmap, vmin=0, vmax=13, aspect='auto')\n",
    "    axes[1].set_title(f'{title_prefix}Predictions\\nFrame {time_frame}', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Azimuth bins (J)', fontsize=11)\n",
    "    axes[1].set_ylabel('Elevation bins (I)', fontsize=11)\n",
    "    axes[1].grid(True, alpha=0.3, color='gray', linewidth=0.5)\n",
    "    \n",
    "    # Plot difference (correct=green, incorrect=red, background=white)\n",
    "    difference = (gt_classes == pred_classes).astype(int)\n",
    "    # Mask background cells\n",
    "    is_background = (gt_classes == 13)\n",
    "    difference[is_background] = 2  # Special value for background\n",
    "    \n",
    "    diff_grid = difference.reshape(I, J)\n",
    "    diff_cmap = plt.matplotlib.colors.ListedColormap(['red', 'green', 'lightgray'])\n",
    "    im3 = axes[2].imshow(diff_grid, cmap=diff_cmap, vmin=0, vmax=2, aspect='auto')\n",
    "    axes[2].set_title(f'{title_prefix}Comparison\\nFrame {time_frame}\\n(Green=Correct, Red=Wrong, Gray=Background)', \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[2].set_xlabel('Azimuth bins (J)', fontsize=11)\n",
    "    axes[2].set_ylabel('Elevation bins (I)', fontsize=11)\n",
    "    axes[2].grid(True, alpha=0.3, color='gray', linewidth=0.5)\n",
    "    \n",
    "    # Add colorbars\n",
    "    cbar1 = plt.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "    cbar1.set_label('Class ID', fontsize=10)\n",
    "    \n",
    "    cbar2 = plt.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "    cbar2.set_label('Class ID', fontsize=10)\n",
    "    \n",
    "    # Calculate accuracy (excluding background)\n",
    "    non_bg_mask = ~is_background\n",
    "    if non_bg_mask.sum() > 0:\n",
    "        accuracy = (gt_classes[non_bg_mask] == pred_classes[non_bg_mask]).mean() * 100\n",
    "        bg_accuracy = (gt_classes[is_background] == pred_classes[is_background]).mean() * 100\n",
    "    else:\n",
    "        accuracy = 0.0\n",
    "        bg_accuracy = (gt_classes == pred_classes).mean() * 100\n",
    "    \n",
    "    # Add statistics text\n",
    "    stats_text = f\"Non-BG Accuracy: {accuracy:.1f}%\\nBG Accuracy: {bg_accuracy:.1f}%\\n\"\n",
    "    stats_text += f\"Active Events: {non_bg_mask.sum()}/{len(gt_classes)}\"\n",
    "    fig.text(0.5, 0.02, stats_text, ha='center', fontsize=12, \n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.08, 1, 1])\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "        logger.info(f\"Visualization saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c307f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(\n",
    "    test_audio_files,\n",
    "    test_meta_files,\n",
    "    model_path=None,\n",
    "    batch_size=None,\n",
    "    device=None,\n",
    "    num_visualizations=5,\n",
    "    save_visualizations=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Test a trained SMR-SELD model and visualize predictions.\n",
    "    \n",
    "    This function:\n",
    "    1. Loads the best model weights\n",
    "    2. Creates predictions and computes loss on test set\n",
    "    3. Visualizes ground truth vs predictions for random frames with active events\n",
    "    \n",
    "    Args:\n",
    "        test_audio_files: List of test audio file paths\n",
    "        test_meta_files: List of test metadata file paths\n",
    "        model_path: Path to model checkpoint (default: best_model.pth)\n",
    "        batch_size: Batch size for testing (default: from config)\n",
    "        device: Device to test on (default: DEVICE global)\n",
    "        num_visualizations: Number of frames to visualize (default: 5)\n",
    "        save_visualizations: Whether to save visualization images (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        results: Dictionary containing test metrics and visualizations\n",
    "    \"\"\"\n",
    "    batch_size = batch_size or config.BATCH_SIZE\n",
    "    device = device or DEVICE\n",
    "    model_path = model_path or (config.CHECKPOINT_PATH / \"best_model.pth\")\n",
    "    \n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(\"STARTING MODEL TESTING\")\n",
    "    logger.info(\"=\"*80)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 1: Load test dataset\n",
    "    # ========================================================================\n",
    "    logger.info(\"\\nStep 1: Loading test dataset...\")\n",
    "    \n",
    "    test_dataset = SELDDataset(\n",
    "        audio_files=test_audio_files,\n",
    "        metadata_files=test_meta_files,\n",
    "        num_classes=config.NUM_CLASSES\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Test dataset: {len(test_dataset)} windows ({len(test_loader)} batches)\")\n",
    "    logger.info(f\"Grid dimensions: {test_dataset.I}x{test_dataset.J} = {test_dataset.total_cells} cells\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 2: Load model\n",
    "    # ========================================================================\n",
    "    logger.info(f\"\\nStep 2: Loading model from {model_path}...\")\n",
    "    \n",
    "    # Check if checkpoint exists\n",
    "    if not Path(model_path).exists():\n",
    "        raise FileNotFoundError(f\"Model checkpoint not found: {model_path}\")\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SMRSELDWithCSPDarkNet(\n",
    "        n_channels=config.N_CHANNELS,\n",
    "        grid_size=(test_dataset.I, test_dataset.J),\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        use_small=True  # Adjust if needed\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    logger.info(f\"Model loaded successfully!\")\n",
    "    logger.info(f\"  Checkpoint epoch: {checkpoint['epoch']}\")\n",
    "    logger.info(f\"  Checkpoint test loss: {checkpoint['test_loss']:.6f}\")\n",
    "    logger.info(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 3: Initialize loss function\n",
    "    # ========================================================================\n",
    "    criterion = SMRSELDLoss(\n",
    "        w_class=config.W_CLASS,\n",
    "        w_aiur=config.W_AIUR,\n",
    "        w_cl=config.W_CL,\n",
    "        grid_size=(test_dataset.I, test_dataset.J)\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 4: Run inference on test set\n",
    "    # ========================================================================\n",
    "    logger.info(\"\\nStep 3: Running inference on test set...\")\n",
    "    \n",
    "    test_loss_accum = 0.0\n",
    "    test_class_mse_accum = 0.0\n",
    "    test_aiur_accum = 0.0\n",
    "    test_cl_accum = 0.0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_window_indices = []\n",
    "    \n",
    "    test_progress = tqdm(test_loader, desc=\"Testing\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (spectrograms, labels) in enumerate(test_progress):\n",
    "            # Move to device\n",
    "            spectrograms = spectrograms.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(spectrograms)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss, breakdown = criterion(predictions, labels)\n",
    "            \n",
    "            # Accumulate losses\n",
    "            test_loss_accum += loss.item()\n",
    "            test_class_mse_accum += breakdown['class_mse']\n",
    "            test_aiur_accum += breakdown['aiur']\n",
    "            test_cl_accum += breakdown['cl']\n",
    "            \n",
    "            # Store predictions and labels for visualization\n",
    "            all_predictions.append(predictions.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_window_indices.extend(range(batch_idx * batch_size, \n",
    "                                           batch_idx * batch_size + spectrograms.shape[0]))\n",
    "            \n",
    "            # Update progress bar\n",
    "            test_progress.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    # Average test losses\n",
    "    avg_test_loss = test_loss_accum / len(test_loader)\n",
    "    avg_test_class_mse = test_class_mse_accum / len(test_loader)\n",
    "    avg_test_aiur = test_aiur_accum / len(test_loader)\n",
    "    avg_test_cl = test_cl_accum / len(test_loader)\n",
    "    \n",
    "    # Concatenate all predictions and labels\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)  # (N, T, G, M)\n",
    "    all_labels = torch.cat(all_labels, dim=0)  # (N, T, G, M)\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"TEST RESULTS\")\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(f\"Total Loss: {avg_test_loss:.6f}\")\n",
    "    logger.info(f\"  Class MSE:  {avg_test_class_mse:.6f}\")\n",
    "    logger.info(f\"  AIUR Loss:  {avg_test_aiur:.6f}\")\n",
    "    logger.info(f\"  CL Loss:    {avg_test_cl:.6f}\")\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    pred_classes = torch.argmax(all_predictions, dim=-1)\n",
    "    true_classes = torch.argmax(all_labels, dim=-1)\n",
    "    overall_accuracy = (pred_classes == true_classes).float().mean().item() * 100\n",
    "    \n",
    "    # Calculate non-background accuracy\n",
    "    is_background = (true_classes == config.NUM_CLASSES - 1)\n",
    "    non_bg_mask = ~is_background\n",
    "    if non_bg_mask.sum() > 0:\n",
    "        non_bg_accuracy = (pred_classes[non_bg_mask] == true_classes[non_bg_mask]).float().mean().item() * 100\n",
    "    else:\n",
    "        non_bg_accuracy = 0.0\n",
    "    \n",
    "    logger.info(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")\n",
    "    logger.info(f\"Non-Background Accuracy: {non_bg_accuracy:.2f}%\")\n",
    "    logger.info(f\"Active Events: {non_bg_mask.sum().item()} / {non_bg_mask.numel()}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 5: Find frames with active events for visualization\n",
    "    # ========================================================================\n",
    "    logger.info(f\"\\nStep 4: Finding frames with active events for visualization...\")\n",
    "    \n",
    "    # Find frames with non-background events\n",
    "    N, T, G, M = all_predictions.shape\n",
    "    frames_with_events = []\n",
    "    \n",
    "    for window_idx in range(N):\n",
    "        for time_idx in range(T):\n",
    "            # Check if this frame has any non-background events\n",
    "            frame_labels = all_labels[window_idx, time_idx, :, :]  # (G, M)\n",
    "            frame_classes = torch.argmax(frame_labels, dim=-1)  # (G,)\n",
    "            \n",
    "            # Count non-background cells\n",
    "            num_active = (frame_classes != config.NUM_CLASSES - 1).sum().item()\n",
    "            \n",
    "            if num_active > 0:\n",
    "                frames_with_events.append({\n",
    "                    'window_idx': window_idx,\n",
    "                    'time_idx': time_idx,\n",
    "                    'num_active': num_active\n",
    "                })\n",
    "    \n",
    "    logger.info(f\"Found {len(frames_with_events)} frames with active events\")\n",
    "    \n",
    "    if len(frames_with_events) == 0:\n",
    "        logger.warning(\"No frames with active events found! Cannot create visualizations.\")\n",
    "        return {\n",
    "            'test_loss': avg_test_loss,\n",
    "            'class_mse': avg_test_class_mse,\n",
    "            'aiur': avg_test_aiur,\n",
    "            'cl': avg_test_cl,\n",
    "            'overall_accuracy': overall_accuracy,\n",
    "            'non_bg_accuracy': non_bg_accuracy,\n",
    "            'visualizations': []\n",
    "        }\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 6: Visualize random frames with active events\n",
    "    # ========================================================================\n",
    "    logger.info(f\"\\nStep 5: Creating {num_visualizations} visualizations...\")\n",
    "    \n",
    "    # Randomly select frames for visualization\n",
    "    num_to_visualize = min(num_visualizations, len(frames_with_events))\n",
    "    selected_frames = random.sample(frames_with_events, num_to_visualize)\n",
    "    \n",
    "    # Sort by number of active events (most interesting first)\n",
    "    selected_frames = sorted(selected_frames, key=lambda x: x['num_active'], reverse=True)\n",
    "    \n",
    "    visualizations = []\n",
    "    \n",
    "    for viz_idx, frame_info in enumerate(selected_frames):\n",
    "        window_idx = frame_info['window_idx']\n",
    "        time_idx = frame_info['time_idx']\n",
    "        num_active = frame_info['num_active']\n",
    "        \n",
    "        logger.info(f\"\\n  Visualization {viz_idx + 1}/{num_to_visualize}:\")\n",
    "        logger.info(f\"    Window: {window_idx}, Time Frame: {time_idx}\")\n",
    "        logger.info(f\"    Active Events: {num_active}\")\n",
    "        \n",
    "        # Get predictions and labels for this frame\n",
    "        frame_predictions = all_predictions[window_idx, time_idx, :, :]  # (G, M)\n",
    "        frame_labels = all_labels[window_idx, time_idx, :, :]  # (G, M)\n",
    "        \n",
    "        # Create visualization\n",
    "        save_path = None\n",
    "        if save_visualizations:\n",
    "            viz_dir = config.OUTPUT_PATH / \"test_visualizations\"\n",
    "            viz_dir.mkdir(exist_ok=True)\n",
    "            save_path = viz_dir / f\"test_viz_{viz_idx + 1}_window{window_idx}_frame{time_idx}.png\"\n",
    "        \n",
    "        fig = visualize_grid_predictions(\n",
    "            ground_truth=frame_labels,\n",
    "            predictions=frame_predictions,\n",
    "            time_frame=time_idx,\n",
    "            grid_size=(test_dataset.I, test_dataset.J),\n",
    "            title_prefix=f\"Window {window_idx}, \",\n",
    "            save_path=save_path\n",
    "        )\n",
    "        \n",
    "        visualizations.append({\n",
    "            'window_idx': window_idx,\n",
    "            'time_idx': time_idx,\n",
    "            'num_active': num_active,\n",
    "            'figure': fig,\n",
    "            'save_path': save_path\n",
    "        })\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"TESTING COMPLETE\")\n",
    "    logger.info(\"=\"*80)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Return results\n",
    "    # ========================================================================\n",
    "    results = {\n",
    "        'test_loss': avg_test_loss,\n",
    "        'class_mse': avg_test_class_mse,\n",
    "        'aiur': avg_test_aiur,\n",
    "        'cl': avg_test_cl,\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'non_bg_accuracy': non_bg_accuracy,\n",
    "        'num_frames_with_events': len(frames_with_events),\n",
    "        'visualizations': visualizations,\n",
    "        'checkpoint_epoch': checkpoint['epoch']\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0327dd41",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🚀 Full Training Pipeline\n",
    "\n",
    "Run the complete end-to-end pipeline: load all data, train model, test model, and save weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65bddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FULL TRAINING AND TESTING PIPELINE\n",
    "====================================\n",
    "\n",
    "This cell runs the complete end-to-end pipeline:\n",
    "1. Loads all training and test audio/metadata files\n",
    "2. Creates train and test datasets\n",
    "3. Trains the model with full configuration\n",
    "4. Tests the model and creates visualizations\n",
    "5. Saves the best model weights for future inference\n",
    "\n",
    "Prerequisites:\n",
    "- All data files should be loaded (train_audio_files, train_meta_files, etc.)\n",
    "- Config class should be properly configured\n",
    "- All functions (train_model, test_model) should be defined\n",
    "\n",
    "WARNING: This will take significant time depending on:\n",
    "   - Dataset size (number of files)\n",
    "   - Number of epochs (config.NUM_EPOCHS)\n",
    "   - Hardware (GPU vs CPU)\n",
    "   \n",
    "Estimated time: Several hours on GPU, much longer on CPU\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING FULL TRAINING AND TESTING PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration Check\n",
    "# ============================================================================\n",
    "print(\"\\n[Configuration]\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Number of epochs: {config.NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"  Early stopping patience: {config.PATIENCE}\")\n",
    "print(f\"  Loss weights: Class={config.W_CLASS}, AIUR={config.W_AIUR}, CL={config.W_CL}\")\n",
    "print(f\"  Checkpoint path: {config.CHECKPOINT_PATH}\")\n",
    "print(f\"  Output path: {config.OUTPUT_PATH}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Verify data files are loaded\n",
    "# ============================================================================\n",
    "print(\"\\n[Step 1] Verifying data files...\")\n",
    "\n",
    "if 'train_audio_files' not in locals() or 'train_meta_files' not in locals():\n",
    "    raise RuntimeError(\"Training files not loaded! Please run the data loading cells first.\")\n",
    "\n",
    "if 'test_audio_files' not in locals() or 'test_meta_files' not in locals():\n",
    "    raise RuntimeError(\"Test files not loaded! Please run the data loading cells first.\")\n",
    "\n",
    "print(f\"Training files: {len(train_audio_files)} audio, {len(train_meta_files)} metadata\")\n",
    "print(f\"Test files: {len(test_audio_files)} audio, {len(test_meta_files)} metadata\")\n",
    "\n",
    "# Verify files exist\n",
    "print(\"\\nVerifying file existence (sampling first 3 files)...\")\n",
    "for i, (audio, meta) in enumerate(zip(train_audio_files[:3], train_meta_files[:3])):\n",
    "    audio_exists = Path(audio).exists()\n",
    "    meta_exists = Path(meta).exists()\n",
    "    status = \"✓\" if audio_exists and meta_exists else \"✗\"\n",
    "    print(f\"  {status} Train {i+1}: Audio={audio_exists}, Meta={meta_exists}\")\n",
    "\n",
    "for i, (audio, meta) in enumerate(zip(test_audio_files[:3], test_meta_files[:3])):\n",
    "    audio_exists = Path(audio).exists()\n",
    "    meta_exists = Path(meta).exists()\n",
    "    status = \"✓\" if audio_exists and meta_exists else \"✗\"\n",
    "    print(f\"  {status} Test {i+1}: Audio={audio_exists}, Meta={meta_exists}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Train the model\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: TRAINING THE MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n This may take several hours depending on dataset size and hardware...\")\n",
    "print(\"Progress will be displayed with loss values for each epoch\")\n",
    "print(\"Model checkpoints will be saved periodically\\n\")\n",
    "\n",
    "# Confirm before starting (optional - comment out if running in batch mode)\n",
    "# response = input(\"Ready to start training? (yes/no): \")\n",
    "# if response.lower() != 'yes':\n",
    "#     print(\"Training cancelled.\")\n",
    "#     raise RuntimeError(\"Training cancelled by user\")\n",
    "\n",
    "try:\n",
    "    # Start training\n",
    "    trained_model, training_history = train_model(\n",
    "        train_audio_files=train_audio_files,\n",
    "        train_meta_files=train_meta_files,\n",
    "        test_audio_files=test_audio_files,\n",
    "        test_meta_files=test_meta_files,\n",
    "        num_epochs=config.NUM_EPOCHS,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        device=DEVICE,\n",
    "        use_small_model=True  # Set to False for full CSPDarkNet53 model\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Display training summary\n",
    "    print(\"\\n[Training Summary]\")\n",
    "    print(f\"  Total epochs trained: {training_history['total_epochs']}\")\n",
    "    print(f\"  Best epoch: {training_history['best_epoch']}\")\n",
    "    print(f\"  Best test loss: {training_history['best_test_loss']:.6f}\")\n",
    "    print(f\"  Final train loss: {training_history['train_losses'][-1]:.6f}\")\n",
    "    print(f\"  Final test loss: {training_history['test_losses'][-1]:.6f}\")\n",
    "    \n",
    "    # Check learning progress\n",
    "    train_improvement = training_history['train_losses'][0] - training_history['train_losses'][-1]\n",
    "    test_improvement = training_history['test_losses'][0] - training_history['test_losses'][-1]\n",
    "    \n",
    "    print(f\"\\n[Learning Progress]\")\n",
    "    print(f\"  Train loss improvement: {train_improvement:.6f} ({train_improvement/training_history['train_losses'][0]*100:.2f}%)\")\n",
    "    print(f\"  Test loss improvement: {test_improvement:.6f} ({test_improvement/training_history['test_losses'][0]*100:.2f}%)\")\n",
    "    \n",
    "    if train_improvement > 0:\n",
    "        print(\"  Model successfully learned from training data\")\n",
    "    else:\n",
    "        print(\"  Training loss did not improve - consider adjusting hyperparameters\")\n",
    "    \n",
    "    if test_improvement > 0:\n",
    "        print(\"  Model generalizes well to test data\")\n",
    "    else:\n",
    "        print(\"  Test loss did not improve - possible overfitting or insufficient training\")\n",
    "    \n",
    "    # Model checkpoint info\n",
    "    best_model_path = config.CHECKPOINT_PATH / \"best_model.pth\"\n",
    "    print(f\"\\n[Model Checkpoints]\")\n",
    "    print(f\"  Best model saved at: {best_model_path}\")\n",
    "    print(f\"  File size: {best_model_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "    print(f\"  Loss curves saved in: {config.OUTPUT_PATH}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING FAILED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nError: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nPlease check the error message above and fix any issues.\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Test the trained model\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: TESTING THE MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nRunning inference on test set...\")\n",
    "print(\"Creating visualizations for frames with active events...\\n\")\n",
    "\n",
    "try:\n",
    "    # Run testing with visualizations\n",
    "    test_results = test_model(\n",
    "        test_audio_files=test_audio_files,\n",
    "        test_meta_files=test_meta_files,\n",
    "        model_path=config.CHECKPOINT_PATH / \"best_model.pth\",\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        device=DEVICE,\n",
    "        num_visualizations=10,  # Create 10 visualizations\n",
    "        save_visualizations=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✅ TESTING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Display test summary\n",
    "    print(\"\\n[Test Performance]\")\n",
    "    print(f\"  Test Loss: {test_results['test_loss']:.6f}\")\n",
    "    print(f\"    - Class MSE:  {test_results['class_mse']:.6f}\")\n",
    "    print(f\"    - AIUR Loss:  {test_results['aiur']:.6f}\")\n",
    "    print(f\"    - CL Loss:    {test_results['cl']:.6f}\")\n",
    "    \n",
    "    print(f\"\\n[Accuracy Metrics]\")\n",
    "    print(f\"  Overall Accuracy: {test_results['overall_accuracy']:.2f}%\")\n",
    "    print(f\"  Non-Background Accuracy: {test_results['non_bg_accuracy']:.2f}%\")\n",
    "    print(f\"  Frames with events: {test_results['num_frames_with_events']}\")\n",
    "    \n",
    "    print(f\"\\n[Visualizations]\")\n",
    "    print(f\"  Number created: {len(test_results['visualizations'])}\")\n",
    "    if len(test_results['visualizations']) > 0:\n",
    "        viz_dir = test_results['visualizations'][0]['save_path'].parent\n",
    "        print(f\"  Saved to: {viz_dir}\")\n",
    "        print(f\"  Files:\")\n",
    "        for viz in test_results['visualizations']:\n",
    "            print(f\"    - {viz['save_path'].name} (Window {viz['window_idx']}, Frame {viz['time_idx']}, {viz['num_active']} events)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TESTING FAILED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nError: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nNote: Training may have succeeded even if testing failed.\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# Final Summary\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n[Summary]\")\n",
    "print(\"Data Loading: PASSED\")\n",
    "print(\"Model Training: PASSED\")\n",
    "print(\"Model Testing: PASSED\")\n",
    "print(\"Visualizations: CREATED\")\n",
    "print(\"Model Weights: SAVED\")\n",
    "\n",
    "print(\"\\n[Output Files]\")\n",
    "print(f\"Checkpoints: {config.CHECKPOINT_PATH}\")\n",
    "print(f\"   - best_model.pth (use this for inference)\")\n",
    "print(f\"   - checkpoint_epoch_*.pth (periodic checkpoints)\")\n",
    "print(f\"Results: {config.OUTPUT_PATH}\")\n",
    "print(f\"   - loss_curves_*.png (training progress)\")\n",
    "print(f\"   - training_history_*.pth (complete training history)\")\n",
    "print(f\"   - test_visualizations/ (prediction visualizations)\")\n",
    "\n",
    "print(\"\\n[Next Steps]\")\n",
    "print(\"1. Review loss curves to understand training dynamics\")\n",
    "print(\"2. Examine test visualizations to see model predictions\")\n",
    "print(\"3. Use best_model.pth for inference on new data\")\n",
    "print(\"4. If results are unsatisfactory, adjust hyperparameters and retrain\")\n",
    "\n",
    "print(\"\\n[Model Info]\")\n",
    "print(f\"  Architecture: SMRSELDWithCSPDarkNet\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in trained_model.parameters()):,}\")\n",
    "print(f\"  Best epoch: {training_history['best_epoch']}/{training_history['total_epochs']}\")\n",
    "print(f\"  Best test loss: {training_history['best_test_loss']:.6f}\")\n",
    "print(f\"  Test accuracy: {test_results['non_bg_accuracy']:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training pipeline completed! Your model is ready for inference.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52665eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_with_gaussian_noise(metadata_path, audio_duration, sample_rate=24000, I=None, J=None, \n",
    "                                cell_size_deg=None, num_classes=14, sigma_azimuth=5.0, sigma_elevation=5.0):\n",
    "    \"\"\"\n",
    "    Convert metadata file to target labels with Gaussian spatial augmentation for each source.\n",
    "    Instead of treating sources as points, treat them as 3D regions with Gaussian distribution.\n",
    "    \"\"\"\n",
    "    if cell_size_deg is None:\n",
    "        cell_size_deg = config.GRID_CELL_DEGREES\n",
    "    \n",
    "    frame_duration_ms = 20\n",
    "    metadata_frame_duration_ms = 100\n",
    "    frames_per_metadata_frame = metadata_frame_duration_ms // frame_duration_ms  # = 5\n",
    "\n",
    "    total_frames = int((audio_duration * 1000) / frame_duration_ms)\n",
    "\n",
    "    if (I is None or J is None) and cell_size_deg is not None:\n",
    "        I = int(180 // cell_size_deg)\n",
    "        J = int(360 // cell_size_deg)\n",
    "    elif I is None or J is None:\n",
    "        raise ValueError(\"Either provide (I, J) or cell_size_deg for grid dimensions\")\n",
    "    \n",
    "    total_cells = I * J\n",
    "\n",
    "    labels = torch.zeros((total_frames, total_cells, num_classes), dtype=torch.float32)\n",
    "    \n",
    "    df = pd.read_csv(metadata_path, header=None)\n",
    "    active_cells_per_frame = [set() for _ in range(total_frames)]\n",
    "    \n",
    "    # Step 1: Identify unique sources in the file\n",
    "    # Source is identified by (class_idx, source_num) tuple\n",
    "    unique_sources = df.groupby([1, 2]).first().reset_index()  # Columns 1=class, 2=source_num\n",
    "    \n",
    "    # Step 2: Generate fixed Gaussian noise for each unique source\n",
    "    # This ensures the same source has consistent spatial augmentation across all frames\n",
    "    source_noise = {}\n",
    "    for _, source_row in unique_sources.iterrows():\n",
    "        class_idx = int(source_row.iloc[0])\n",
    "        source_num = int(source_row.iloc[1])\n",
    "        source_key = (class_idx, source_num)\n",
    "\n",
    "        azimuth_noise = np.random.normal(0, sigma_azimuth)\n",
    "        elevation_noise = np.random.normal(0, sigma_elevation)\n",
    "        \n",
    "        source_noise[source_key] = (azimuth_noise, elevation_noise)\n",
    "    \n",
    "    # Step 3: Process each row in metadata with augmentation\n",
    "    for _, row in df.iterrows():\n",
    "        metadata_frame = int(row.iloc[0])\n",
    "        active_class = int(row.iloc[1])\n",
    "        source_num = int(row.iloc[2])\n",
    "        azimuth = int(row.iloc[3])\n",
    "        elevation = int(row.iloc[4])\n",
    "        \n",
    "        source_key = (active_class, source_num)\n",
    "        \n",
    "        # Get the fixed Gaussian noise for this source\n",
    "        azimuth_noise, elevation_noise = source_noise[source_key]\n",
    "        \n",
    "        # Map metadata frame to final representation frames\n",
    "        start_frame = metadata_frame * frames_per_metadata_frame\n",
    "        end_frame = start_frame + frames_per_metadata_frame\n",
    "        end_frame = min(end_frame, total_frames)\n",
    "        \n",
    "        # Step 4: Create Gaussian region around the source location\n",
    "        # Instead of sampling points, consider the entire bounded region\n",
    "        # Use 2-sigma range (covers ~95% of Gaussian distribution)\n",
    "        # This creates a bounded rectangular region in azimuth-elevation space\n",
    "        \n",
    "        affected_cells = set()\n",
    "        \n",
    "        # Define the bounded region using the Gaussian noise and 2-sigma bounds\n",
    "        # The region center is offset by the fixed noise for this source\n",
    "        center_azimuth = azimuth + azimuth_noise\n",
    "        center_elevation = elevation + elevation_noise\n",
    "        \n",
    "        # Define bounds: use 2*sigma to cover ~95% of the Gaussian\n",
    "        # This creates a rectangular region in the angular space\n",
    "        azimuth_min = center_azimuth - 2 * sigma_azimuth\n",
    "        azimuth_max = center_azimuth + 2 * sigma_azimuth\n",
    "        elevation_min = center_elevation - 2 * sigma_elevation\n",
    "        elevation_max = center_elevation + 2 * sigma_elevation\n",
    "        \n",
    "        # Clip elevation to valid range\n",
    "        elevation_min = max(elevation_min, -90)\n",
    "        elevation_max = min(elevation_max, 90)\n",
    "        \n",
    "        # Find all grid cells that fall within this bounded region\n",
    "        # We need to check each grid cell to see if it falls within the bounds\n",
    "        for grid_i in range(I):\n",
    "            for grid_j in range(J):\n",
    "                # Get the center coordinates of this grid cell\n",
    "                # Grid cell (i, j) corresponds to:\n",
    "                # elevation: -90 + (i + 0.5) * cell_size_elevation\n",
    "                # azimuth: -180 + (j + 0.5) * cell_size_azimuth\n",
    "                cell_size_elevation = 180.0 / I\n",
    "                cell_size_azimuth = 360.0 / J\n",
    "                \n",
    "                cell_elevation = -90 + (grid_i + 0.5) * cell_size_elevation\n",
    "                cell_azimuth = -180 + (grid_j + 0.5) * cell_size_azimuth\n",
    "                \n",
    "                # Check if this cell's center falls within the Gaussian region bounds\n",
    "                # For azimuth, need to handle wraparound at -180/180\n",
    "                # Normalize all azimuths to be in the same reference frame\n",
    "                def normalize_azimuth_diff(az1, az2):\n",
    "                    \"\"\"Calculate shortest angular distance between two azimuths\"\"\"\n",
    "                    diff = az1 - az2\n",
    "                    while diff > 180:\n",
    "                        diff -= 360\n",
    "                    while diff < -180:\n",
    "                        diff += 360\n",
    "                    return diff\n",
    "                \n",
    "                # Check if cell is within azimuth bounds (considering wraparound)\n",
    "                azimuth_dist = abs(normalize_azimuth_diff(cell_azimuth, center_azimuth))\n",
    "                azimuth_in_bounds = azimuth_dist <= 2 * sigma_azimuth\n",
    "                \n",
    "                # Check if cell is within elevation bounds (simple range check)\n",
    "                elevation_in_bounds = elevation_min <= cell_elevation <= elevation_max\n",
    "                \n",
    "                if azimuth_in_bounds and elevation_in_bounds:\n",
    "                    cell_idx = grid_i * J + grid_j\n",
    "                    affected_cells.add(cell_idx)\n",
    "        \n",
    "        # Step 5: Set active class for all affected cells in the region\n",
    "        for cell_idx in affected_cells:\n",
    "            for t in range(start_frame, end_frame):\n",
    "                # Use softer probability for cells in the region\n",
    "                # Central cells (from original position) get higher probability\n",
    "                labels[t, cell_idx, active_class] = 1.0\n",
    "                active_cells_per_frame[t].add(cell_idx)\n",
    "    \n",
    "    # Step 6: Set background class for cells with no active events\n",
    "    for t in range(total_frames):\n",
    "        for cell_idx in range(total_cells):\n",
    "            if cell_idx not in active_cells_per_frame[t]:\n",
    "                labels[t, cell_idx, num_classes - 1] = 1.0\n",
    "    \n",
    "    return labels, I, J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0eed5d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_augmentation_methods(metadata_path, audio_duration):\n",
    "    \"\"\"\n",
    "    Compare the original point-wise labeling with Gaussian region augmentation.\n",
    "    \n",
    "    Args:\n",
    "        metadata_path: Path to metadata CSV file\n",
    "        audio_duration: Duration of audio in seconds\n",
    "    \"\"\"\n",
    "    # Original point-wise labeling\n",
    "    labels_original, I, J = metadata_to_labels(\n",
    "        metadata_path, \n",
    "        audio_duration,\n",
    "        sample_rate=config.SR,\n",
    "        cell_size_deg=config.GRID_CELL_DEGREES,\n",
    "        num_classes=config.NUM_CLASSES\n",
    "    )\n",
    "    \n",
    "    # Augmented region-based labeling\n",
    "    labels_augmented, _, _ = augment_with_gaussian_noise(\n",
    "        metadata_path,\n",
    "        audio_duration,\n",
    "        sample_rate=config.SR,\n",
    "        cell_size_deg=config.GRID_CELL_DEGREES,\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        sigma_azimuth=5.0,  # 5 degrees standard deviation\n",
    "        sigma_elevation=5.0\n",
    "    )\n",
    "    \n",
    "    # Compare statistics\n",
    "    print(\"Comparison of Original vs Augmented Labeling:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Count non-background cells\n",
    "    original_nonbg = (labels_original[:, :, :-1].sum(dim=-1) > 0).sum().item()\n",
    "    augmented_nonbg = (labels_augmented[:, :, :-1].sum(dim=-1) > 0).sum().item()\n",
    "    \n",
    "    print(f\"Original - Active cells (non-background): {original_nonbg}\")\n",
    "    print(f\"Augmented - Active cells (non-background): {augmented_nonbg}\")\n",
    "    print(f\"Increase in active cells: {augmented_nonbg - original_nonbg} ({(augmented_nonbg/original_nonbg - 1)*100:.1f}%)\")\n",
    "    \n",
    "    # Find frames with different characteristics\n",
    "    print(\"\\nFinding representative frames...\")\n",
    "    \n",
    "    # Calculate activity per frame (non-background cells)\n",
    "    activity_per_frame = (labels_original[:, :, :-1].sum(dim=-1) > 0).sum(dim=-1)\n",
    "    \n",
    "    # Find 1 frame with no events\n",
    "    no_event_frames = (activity_per_frame == 0).nonzero(as_tuple=True)[0]\n",
    "    if len(no_event_frames) > 0:\n",
    "        frame_no_event = no_event_frames[len(no_event_frames) // 2].item()  # Pick middle one\n",
    "    else:\n",
    "        frame_no_event = 0\n",
    "        print(\"  Warning: No frames without events found, using frame 0\")\n",
    "    \n",
    "    # Find 2 frames with events (low and high activity)\n",
    "    event_frames = (activity_per_frame > 0).nonzero(as_tuple=True)[0]\n",
    "    if len(event_frames) >= 2:\n",
    "        # Sort by activity level\n",
    "        event_frames_sorted = event_frames[torch.argsort(activity_per_frame[event_frames])]\n",
    "        frame_low_event = event_frames_sorted[len(event_frames_sorted) // 3].item()  # Low activity\n",
    "        frame_high_event = event_frames_sorted[2 * len(event_frames_sorted) // 3].item()  # High activity\n",
    "    elif len(event_frames) == 1:\n",
    "        frame_low_event = event_frames[0].item()\n",
    "        frame_high_event = event_frames[0].item()\n",
    "    else:\n",
    "        frame_low_event = 10\n",
    "        frame_high_event = 20\n",
    "        print(\"  Warning: No frames with events found, using default frames\")\n",
    "    \n",
    "    selected_frames = [\n",
    "        (frame_no_event, \"No Events\", int(activity_per_frame[frame_no_event].item())),\n",
    "        (frame_low_event, \"Low Activity\", int(activity_per_frame[frame_low_event].item())),\n",
    "        (frame_high_event, \"High Activity\", int(activity_per_frame[frame_high_event].item()))\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nSelected frames:\")\n",
    "    for frame_idx, desc, activity in selected_frames:\n",
    "        print(f\"  {desc}: Frame {frame_idx} (Activity: {activity} cells)\")\n",
    "    \n",
    "    # Create output directory\n",
    "    import os\n",
    "    output_dir = \"gaussian_visualizations\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"\\nSaving visualizations to: {output_dir}/\")\n",
    "    \n",
    "    # Visualize each selected frame\n",
    "    for frame_idx, desc, activity in selected_frames:\n",
    "        original_frame = labels_original[frame_idx].reshape(I, J, -1)\n",
    "        augmented_frame = labels_augmented[frame_idx].reshape(I, J, -1)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Original\n",
    "        original_activity = original_frame[:, :, :-1].sum(dim=-1).numpy()\n",
    "        im1 = axes[0].imshow(original_activity, cmap='hot', aspect='auto', origin='lower', vmin=0, vmax=1)\n",
    "        axes[0].set_title(f'Original Point-wise\\nFrame {frame_idx} - {desc} ({activity} cells)', fontsize=12, fontweight='bold')\n",
    "        axes[0].set_xlabel('Azimuth bins (J)')\n",
    "        axes[0].set_ylabel('Elevation bins (I)')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        plt.colorbar(im1, ax=axes[0], label='Activity')\n",
    "        \n",
    "        # Augmented\n",
    "        augmented_activity = augmented_frame[:, :, :-1].sum(dim=-1).numpy()\n",
    "        augmented_cells = (augmented_activity > 0).sum()\n",
    "        im2 = axes[1].imshow(augmented_activity, cmap='hot', aspect='auto', origin='lower', vmin=0, vmax=1)\n",
    "        axes[1].set_title(f'Augmented Gaussian Region\\nFrame {frame_idx} - {desc} ({augmented_cells} cells)', fontsize=12, fontweight='bold')\n",
    "        axes[1].set_xlabel('Azimuth bins (J)')\n",
    "        axes[1].set_ylabel('Elevation bins (I)')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        plt.colorbar(im2, ax=axes[1], label='Activity')\n",
    "        \n",
    "        # Overall title\n",
    "        increase_pct = ((augmented_cells / max(activity, 1)) - 1) * 100 if activity > 0 else 0\n",
    "        fig.suptitle(f'Gaussian Augmentation Comparison - {desc} Frame\\n'\n",
    "                     f'Increase: {augmented_cells - activity} cells (+{increase_pct:.1f}%)',\n",
    "                     fontsize=14, fontweight='bold', y=1.02)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        safe_desc = desc.lower().replace(\" \", \"_\")\n",
    "        save_path = os.path.join(output_dir, f'augmentation_frame_{frame_idx}_{safe_desc}.png')\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"  Saved: {save_path}\")\n",
    "        \n",
    "        plt.close(fig)\n",
    "    \n",
    "    print(\"\\nVisualization complete!\")\n",
    "    \n",
    "    return labels_original, labels_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48222021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Gaussian Augmentation\n",
      "================================================================================\n",
      "\n",
      "Selected file for testing:\n",
      "  Metadata: fold3_room22_mix005.csv\n",
      "  Audio: fold3_room22_mix005.wav\n",
      "  Duration: 133.50 seconds\n",
      "  Sample Rate: 24000 Hz\n",
      "\n",
      "Running Gaussian augmentation comparison...\n",
      "--------------------------------------------------------------------------------\n",
      "Comparison of Original vs Augmented Labeling:\n",
      "============================================================\n",
      "Original - Active cells (non-background): 15415\n",
      "Augmented - Active cells (non-background): 61465\n",
      "Increase in active cells: 46050 (298.7%)\n",
      "\n",
      "Finding representative frames...\n",
      "Comparison of Original vs Augmented Labeling:\n",
      "============================================================\n",
      "Original - Active cells (non-background): 15415\n",
      "Augmented - Active cells (non-background): 61465\n",
      "Increase in active cells: 46050 (298.7%)\n",
      "\n",
      "Finding representative frames...\n",
      "\n",
      "Selected frames:\n",
      "  No Events: Frame 2 (Activity: 0 cells)\n",
      "  Low Activity: Frame 1438 (Activity: 2 cells)\n",
      "  High Activity: Frame 1931 (Activity: 3 cells)\n",
      "\n",
      "Saving visualizations to: gaussian_visualizations/\n",
      "\n",
      "Selected frames:\n",
      "  No Events: Frame 2 (Activity: 0 cells)\n",
      "  Low Activity: Frame 1438 (Activity: 2 cells)\n",
      "  High Activity: Frame 1931 (Activity: 3 cells)\n",
      "\n",
      "Saving visualizations to: gaussian_visualizations/\n",
      "  Saved: gaussian_visualizations\\augmentation_frame_2_no_events.png\n",
      "  Saved: gaussian_visualizations\\augmentation_frame_2_no_events.png\n",
      "  Saved: gaussian_visualizations\\augmentation_frame_1438_low_activity.png\n",
      "  Saved: gaussian_visualizations\\augmentation_frame_1438_low_activity.png\n",
      "  Saved: gaussian_visualizations\\augmentation_frame_1931_high_activity.png\n",
      "\n",
      "Visualization complete!\n",
      "\n",
      "================================================================================\n",
      "Gaussian augmentation test complete!\n",
      "Check the 'gaussian_visualizations/' directory for results.\n",
      "================================================================================\n",
      "  Saved: gaussian_visualizations\\augmentation_frame_1931_high_activity.png\n",
      "\n",
      "Visualization complete!\n",
      "\n",
      "================================================================================\n",
      "Gaussian augmentation test complete!\n",
      "Check the 'gaussian_visualizations/' directory for results.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Gaussian Augmentation on a Single File\n",
    "print(\"Testing Gaussian Augmentation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load a single file from the training set\n",
    "if len(train_meta_files) > 0:\n",
    "    # Pick a file from the middle of the training set\n",
    "    test_file_idx = len(train_meta_files) // 2\n",
    "    test_metadata_path = train_meta_files[test_file_idx]\n",
    "    test_audio_path = train_audio_files[test_file_idx]\n",
    "    \n",
    "    print(f\"\\nSelected file for testing:\")\n",
    "    print(f\"  Metadata: {Path(test_metadata_path).name}\")\n",
    "    print(f\"  Audio: {Path(test_audio_path).name}\")\n",
    "    \n",
    "    # Get audio duration\n",
    "    import torchaudio\n",
    "    audio_info = torchaudio.info(test_audio_path)\n",
    "    audio_duration = audio_info.num_frames / audio_info.sample_rate\n",
    "    \n",
    "    print(f\"  Duration: {audio_duration:.2f} seconds\")\n",
    "    print(f\"  Sample Rate: {audio_info.sample_rate} Hz\")\n",
    "    \n",
    "    # Run comparison and generate visualizations\n",
    "    print(\"\\nRunning Gaussian augmentation comparison...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    labels_original, labels_augmented = compare_augmentation_methods(\n",
    "        test_metadata_path, \n",
    "        audio_duration\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Gaussian augmentation test complete!\")\n",
    "    print(f\"Check the 'gaussian_visualizations/' directory for results.\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "else:\n",
    "    print(\"No training files found. Please load the dataset first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
